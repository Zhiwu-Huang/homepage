<!DOCTYPE html>
<html>
  <head>
    <title>Zhiwu Huang, Computer Vision Lab, ETH Zurich </title>
    <link href='https://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109828585-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-109828585-1');
	</script>
	  
	<script type="text/javascript">
	   function visibility_on(id) {
		var e = document.getElementById(id+"_text");
		if(e.style.display == 'none')
		    e.style.display = 'block';
		var e = document.getElementById(id+"_img");
		if(e.style.display == 'none')
		    e.style.display = 'block';
	   }
	   function visibility_off(id) {
		var e = document.getElementById(id+"_text");
		if(e.style.display == 'block')
		    e.style.display = 'none';
		var e = document.getElementById(id+"_img");
		if(e.style.display == 'block')
		    e.style.display = 'none';
	   }
	   function toggle_visibility(id) {
	       var e = document.getElementById(id+"_text");
	       if(e.style.display == 'inline')
		  e.style.display = 'block';
	       else
		  e.style.display = 'inline';
	       var e = document.getElementById(id+"_img");
	       if(e.style.display == 'inline')
		  e.style.display = 'block';
	       else
		  e.style.display = 'inline';
	   }
	   function toggle_vis(id) {
	       var e = document.getElementById(id);
	       if (e.style.display == 'none')
		   e.style.display = 'inline';
	       else
		   e.style.display = 'none';
	   }
	</script>
	  
	  
	  
    <style>
      body {
      font-family: 'Vollkorn', sans-serif;
      }

      .subheading {
      font-size: 120%;
      }

      h2 {
      clear: left;
      margin-top: 1em;
      }
      
      h3 {
      clear: left;
      margin-top: 1em;
      }
      
      h4 {
      clear: left;
      margin-top: 1em;
      }
      
      img {
      float: left;
      height: 12em;
      margin-right: 1em;
      margin-bottom: 3em;
      }
     

      p {
      margin: 0.5em;
      }

      nav {
      margin-top: 3em;
      margin-bottom: 1em;
      }
        
        a:visited {color:#006633}
        a:link {color:#006633}
        a:hover {color: #FF0000}
        
        h1,h2,h3,h4{
            color:#006633
        }

    </style>
  </head>
  
  <body>
    <table cellspacing="0"><tr><td width=100%>
    <h1>Zhiwu Huang</h1>
    <div class="subheading">
      <img src="ZhiwuHuang-Hawaii.jpg"/>
      I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, ETH Zurich. My research interest lies in Computer Vision and Machine Learning for <b> Automated Video Artificial Intelligence, </b> capable of autmatically learning to understand the world through videos. I currently working on video generation, enhancement and manipulation as well as human-focussed video clustering, classification, prediction with deep manifold learning, generative distribution learning, and neural architecture learning.<br><br>	
      <b> [<a href="http://scholar.google.ch/citations?user=yh6t92AAAAAJ&hl=en">Google Scholar</a>] [<a href="https://github.com/zhiwu-huang"> Github </a>] </b>
      
       <!-- <b>Prospective students </b>, please <a href="javascript:toggle_vis('contact')">read this</a> before contacting me.
              <div id="contact" style="display:none"> 
                  Thank you for your interest in joining my research team! I am taking on new MS and PhD students each year. However, I ask that you do not contact me directly with regard to MS or PhD admissions until after you are admitted, as I will not be able to reply to individual emails. <br>
                  If you are interested in a <i>post-doc</i> position, please read <a href="https://goo.gl/forms/aKL2gnq8T80FNVMb2">this form</a>. <br>
                  If you are a current or admitted <i>Stanford undergraduate or MS student</i> interested in research positions, please read <a href="https://goo.gl/forms/uJuYOfIBQVPhEEDy1">this form</a>. <br>
                  If you are not a Stanford student and insteresed in research positions, please read <a href="https://goo.gl/forms/9scJRH3hw6z7GNbj2">this form</a>.
              </div>
        </p>-->
        
    </div>
    

    <nav>
      Jump to: 
      <a href="index.html#Projects">Research</a> |
      <a href="publication.html">Publications</a> |
      <a href="workshop.html">Workshops </a> |
      <a href="talk.html">Talks</a> |
      <a href="teaching.html">Teaching</a> |
      <a href="about.html">About me</a> | 
      <a href="contact.html">Contact</a>
    </nav>
	    
 
      </td>
    </tr></table>
    
    <h2> My SMU research team is looking for multiple new PhD candidates. </h2> 
	  
    <h3> Research Description </h3>
    
        Videos are ideally suited for teaching both humans and machines lots of knowledge about the world because they implicitly encode the physical constraints that define it. However, existing computer vision models generally approach the video understanding problems with extremely expensive expert knowledge in terms of data pre-processing, data annotation, feature engineering, network design and loss function exploitation. The project aims at addressing some of these challenges for either low-level or high-level video understanding problems. The PhD student tasks will involve developing novel ideas in cutting-​edge research on Automated Video Artificial Intelligence, as well as conducting data collection for their evaluation. 

    <h3> Your profile </h3>
    
	      <li> Masters degree in computer science, electrical engineering, or applied mathematics </li>
    	  <li> High enthusiasm for leading-​edge research, team spirit, and capability of independent problem-solving </li>
        <li> Rich programming knowledge and experience with Python / MATLAB / C++ </li>
        <li> Fluent written and spoken English is a must </li>
        <li> Good knowledge and experience in one or more of the following is a plus: computer vision, deep learning, manifold learning, generative modeling, GPU programming </li>

    <h3> Funding Opportunities </h3>
	  
	  SMU generally offers very competitive scholarships and financial aid to PhD candidates. For more details, please refer to https://sis.smu.edu.sg/programmes/PhD/admission-fees-scholarships

    <h3> How To Apply </h3>
	  
	  For applications, please first send an email to: zhiwu.huang@vision.ee.ethz.ch, including: CV, transcript of courses taken and exam results, and qualifications in reading, speaking, and writing English, and two referee reports (referees are expected to submit the completed referee form directly to us). After the shortlisting and interview process, we will decide to create an initial offer, and suggest the candidates to go for the online application: https://sis.smu.edu.sg/programmes/PhD/online-application 

	   
  <h3>  A Tour of SMU's City Campus </h3>
  
  <table><tr>

	  <td width="40%">  
		  <!--<a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io">
		  <video autoplay muted loop width="100%">
		  <source src="./Videos/trailerfaces_generation_cropped.mp4" type="video/mp4">
		  <source src="./Videos/trailerfaces_generation_cropped.mp4" type="video/mp4">
		  Sorry, we cannot display the generated video wall as
		  your browser doesn't support HTML5 video.
	         </video></a>-->
		   <div class="media__wrapper media__video">
			<iframe
			class="media__element"
			allowfullscreen
			src="https://www.youtube.com/embed/IvbhVPLS2fM"></iframe>
		  </div>
	  
	  </td>
	  
	  

	<td valign="top">
		<p><a href="https://github.com/musikisomorphie/swd"> Source Code </a> | <a href="https://data.vision.ee.ethz.ch/zzhiwu/trailerFaces-tfrecords.zip"> TrailerFace Dataset </a> 
		<p> Sliced Wasserstein Generative Models. <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu* </a>, <b>Zhiwu Huang*</b>,  <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <a href="https://scholar.google.com/citations?user=yjG4Eg4AAAAJ&hl=en"> Wen Li </a>, <a href="https://scholar.google.ch/citations?user=NpZj7TAAAAAJ&hl=en"> Janine Thoma </a>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em> (*indicates equal contributions). </em>                
			 In <em> Computer Vision and Pattern Recognition (CVPR), 2019 </em>. <a href="https://arxiv.org/pdf/1706.02631.pdf">Paper</a> | <a href="https://github.com/musikisomorphie/swd"> Code </a></p>
		<p>Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs. <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.  
			 <em>  arXiv:1810.02419, 2018. </em> <a href="https://arxiv.org/pdf/1810.02419.pdf"> Paper </a> | <a href="https://github.com/musikisomorphie/swd"> Code </a> </p>
		<p>Improving Video Generation for Multi-functional Applications. <a href="https://scholar.google.ch/citations?user=zYjOhRUAAAAJ&hl=en"> Bernhard Kratzwald </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>,  <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em>  arXiv:1711.11453, 2017</em>. <a href="https://arxiv.org/pdf/1711.11453.pdf"> Paper </a> | <a href="https://bernhard2202.github.io/ivgan/index.html"> Project Page </a> </p>
		<p>Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search. <a href="https://scholar.google.com/citations?user=84hs7J4AAAAJ&hl=en">Yuan Tian*</a>, <a href="https://scholar.google.ch/citations?user=-8OUEKwAAAAJ&hl=en">Qin Wang*</a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com.sg/citations?user=yjG4Eg4AAAAJ&hl=en">Wen Li</a>, <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://www.linkedin.com/in/minghaoyang/?originalSubdomain=nl"> Minghao Yang </a>, <a href="https://scholar.google.com/citations?user=wIE1tY4AAAAJ&hl=en"> Jun Wang </a>, <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink</a>. <em> (*indicates equal contributions) </em>. In <em> European Conference on Computer Vision (ECCV), 2020 </em>. <a href="https://arxiv.org/pdf/2007.09180.pdf">Paper </a> | <a href="https://www.qin.ee/eccv20.mp4"> Video </a> | <a href="https://github.com/Yuantian013/E2GAN"> Code </a> | <a href="https://openreview.net/forum?id=A1wIiNR_S5&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Dthecvf.com%2FECCV%2F2020%2FConference%2FAuthors%23your-submissions"> OpenReview </a> 

	</td></tr></table>
	  
     
	    
   </body>
</html>
