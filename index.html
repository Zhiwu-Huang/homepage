---
layout: default
---

<head>
<style>
.image-txt-container {
  display:flex;
  align-items:center;
  flex-direction: row;
}
.item-image {
  margin: 0px 20px 0px 0px;
  width: 200px;
}
.profile-image {
  margin: 0px 0px 0px 20px;
  width: 300px;
}
</style>
</head>

<body>

<div class="image-txt-container">
	<div>
<h2>Biography</h2>
I am a postdoctoral researcher under the supervision of Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, ETH Zurich. My research interest lies in Computer Vision and Machine Learning for <b> Automated Video Artificial Intelligence, </b> capable of autmatically learning to understand the world through videos. I currently working on human-focussed video clustering, classification, prediction as well as video generation, enhancement and manipulation with deep manifold learning, generative distribution learning, and neural architecture learning.<br><br>	
<b>[<a href="https://vision.ee.ethz.ch/people-details.MjIwNDU5.TGlzdC8zMjkyLC0xOTcxNDY1MTc4.html">Contact details</a>] [<a href="http://scholar.google.ch/citations?user=yh6t92AAAAAJ&hl=en">Google Scholar</a>] </b>	
	
	</div>

<img src="ZhiwuHuang-Notre-Dame.jpg" class="profile-image">
</div>
	


<div class="lab-collaboration">

In the lab, I am mainly working for the following research projects:
			
			<p> <b> Video Generation and Classification </b> with <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel, </a> <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu, </a> </a> <a href="https://scholar.google.ch/citations?user=pfEoUpcAAAAJ&hl=de"> Thomas Probst </a>  </p>
			<p> <b> Video Enhancement </b> with <a href="https://scholar.google.ch/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte, </a> <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel, </a> <a href="http://www.vision.ee.ethz.ch/en/members/detail/408/"> Dario Fuoli, </a> <a href="https://scholar.google.com/citations?hl=en&user=NCSSpMkAAAAJ"> Martin Danelljan </a>   </p>

			<p> <b> Video Manipulation </b> with <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel, </a> <a href="https://scholar.google.ch/citations?user=3BHMHU4AAAAJ&hl=en&oi=ao"> Ajad Chhatkuli, </a> <a href="https://vision.ee.ethz.ch/people-details.MjY0NDM1.TGlzdC8zMjg5LC0xOTcxNDY1MTc4.html/"> Mohamad Shahbazi </a>  </p>

			<p> <b> Video Clustering </b> with <a href="https://scholar.google.ch/citations?hl=en&user=wbk0QAcAAAAJ"> Suryansh Kumar </a>
			<br>
			<br>

	
</div>

<div class="before">
Before coming to Zurich, I obtained my PhD degree from Visual Information Processing and Learning (VIPL) group in Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS), in 2015. Prof. <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=en">Shiguang Shan</a> is my PhD supervisor. During my PhD study, Prof. <a href="https://scholar.google.com/citations?user=duIUwpwAAAAJ">Ruiping Wang</a> worked as my mentor, and I also worked closely with Prof. <a href="https://scholar.google.com/citations?user=vVx2v20AAAAJ">Xilin Chen</a>.

</div>



<div class="news">
	<br>
	<h2> News</h2>
	<ul>
	 	<li> 05/2020, We are organizing <a href= "https://competitions.codalab.org/competitions/24685"> AIM Video Super-Resolution Challenge </a> @ECCV 2020. </li>
 		<li> 05/2020, One paper "NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results" will appear at the workshop NTIRE in conjunction with CVPR 2020. </li>
    		<li> 12/2019, We are organizing <a href= "https://competitions.codalab.org/competitions/20247"> NTIRE Video Quality Mapping Challenge </a> @CVPR 2020. </li>
    		<li> 10/2019, One dataset paper "The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping" will appear at the workshop AIM in ICCV 2019. </li>
		<li>10/2019, We are organizing a workshop <a href= "http://www.vision.ee.ethz.ch/aim19/"> "AIM: Advances in Image Manipulation workshop and challenges on image and video manipulation" </a>
  and a tutorial <a href= "http://www.vision.ee.ethz.ch/fire19/index_fire19.html"> "FIRE: From Image Restoration to Enhancement and Beyond" </a> at ICCV, Oct. 27, 2019. </li>

		<li>02/2019, One paper "Sliced Wasserstein Generative Models" is accepted by CVPR 2019. <b> This paper was selected as one of the BEST PUBLICATIONS of the week (20.04.2019), by DeepAI. <a  href="https://deepai.org/publication/sliced-wasserstein-generative-models"> Link to DeepAI Website </a>  </b></li>

		<!--<li>11/2018, One paper "Manifold-valued Image Generation with Wasserstein Generative Adversarial Nets" is accepted by AAAI 2019. <em> This paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli (2018 Fields Medal winner) to generative modeling, which is able to generate photo-realistic biological samples. </em> </li>
<!--<b> The paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli, who won 2018 Fields Medal. </b> -->
		<!--<li>07/2018, One paper "Wasserstein Divergence for GANs" is accepted by ECCV 2018.</li>
		<li>04/2018, One paper "Covariance Pooling for Facial Expression Recognition" is accepted by the workshop DiffCVML in CVPR 2018.</li>
		<li>11/2017, One paper "Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video" is accepted as a Regular Paper in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</li>
		<li>11/2017, One paper "Building Deep Networks on Grassmann Manifolds" (GrNet) is accepted by AAAI 2018.</li>
	    <li>08/2017, One paper "Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets" is accepted by IEEE Transactions on Image Processing (TIP).</li>
		<li>07/2017, One paper "Geometry-aware Similarity Learning on SPD Manifolds for Visual Recognition" is accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
		<li>02/2017, One paper "Deep Learning on Lie Groups for Skeleton-based Action Recognition" (LieNet) is accepted as a spotlight by CVPR 2017. <em> This paper proposes deep networks of Lie Groups for skeleton-based action recognition. </em> </li>
	    <li>11/2016, One paper "A Riemannian Network for SPD Matrix Learning" (SPDNet) is accepted by AAAI 2017. <em>  This paper opens a new direction of deep manifold networks. </em> </li>-->
		<!-- <li>03/2016, Source code of our ICML 2015 paper is available.
		<li>01/2016, Source code of our CVPR 2015 paper is available.-->
        <!--<li>05/2015, One paper is accepted by IEEE Transactions on Image Processing (TIP).</li>
		<li>05/2015, One paper is accepted by ICML 2015.</li>
	    <li>03/2015, Three papers are accepted by CVPR 2015.</li> 		
        <li>03/2015, One paper is accepted by Pattern Recognition (PR).</li> -->		
     </ul>
</div>


<div class="container">
		<h2>Selected Preprints</h2>
	

		<div class="publication">
		       <img src="dacal.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a  href="https://arxiv.org/pdf/1910.10455.pdf">
			Divide-and-Conquer Adversarial Learning for High-Resolution Image and Video Enhancement 
		     </a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Danda Pani Paudel, Guanju Li, Jiqing Wu, Radu Timofte, Luc Van Gool.  
				<br>
				<em>  
					<a href="https://arxiv.org/abs/1910.10455"> 
					arXiv:1910.10455</a>, 2019  </em>    
				
			    <br>	
              </p>

        	</div>
	
	
		


		<div class="publication">
		       <img src="proVid2.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a  href="https://arxiv.org/pdf/1810.02419.pdf"> Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs </a>
                </strong>
                <br>
                <br>
                Dinesh Acharya, <b>Zhiwu Huang</b>, Danda Pani Paudel, Luc Van Gool.  
				<br>
				<em>  <a href="https://arxiv.org/abs/1810.02419"> arXiv:1810.02419 </a>, 2018 </em>    
				
			    <br>	
              </p>
        	</div>

		
		
		
		
		
		
		<!--<div class="publication">
		       <img src="figs/ivideogan.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1711.11453.pdf">Improving Video Generation for Multi-functional Applications </a>
                </strong>
                <br>
                <br>
                Bernhard Kratzwald, <b>Zhiwu Huang</b>, Danda Pani Paudel, Dinesh Acharya, Luc Van Gool.  
				<br>
				<em>  <a href="https://arxiv.org/abs/1711.11453"> arXiv:1711.11453 </a>,  <a href="https://bernhard2202.github.io/ivgan/index.html"> Project Page </a> 2017 </em>     
				
			    <br>	
              </p>
        </div>
		
		
		<div class="publication">
		       <img src="figs/idcyclegan.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1712.00971.pdf"> Face Translation between Images and Videos using Identity-aware CycleGAN</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Bernhard Kratzwald, Danda Pani Paudel, Jiqing Wu, Luc Van Gool.  
				<br>
				<em>  <a href="https://arxiv.org/abs/1712.00971"> arXiv:1712.00971 </a>, 2017 </em>                
			    <br>	
              </p>
        </div>
		

		
		
		
		<div class="publication">
		       <img src="figs/3DR-arxiv.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/abs/1704.01372">On the Relation between Color Image Denoising and Classification</a>
                </strong>
                <br>
                <br>
                 Jiqing Wu, Radu Timofte, <b>Zhiwu Huang</b>, Luc Van Gool.  
				<br>
				<em>  <a href="https://arxiv.org/abs/1704.01372"> arXiv:1611.05742</a>, 2017 </em>                
			    <br>	
        </div>-->
		 
		 
			  
    </div>

	<!--<div class="container">
	 </div>-->

	<div class="container">
		<h2>Selected Conference Papers</h2>
		
	
		<div class="image-txt-container">
		<img src="vqm_ntire2020_new.gif" class="item-image">
			<div>
			<h3> <a href="https://arxiv.org/pdf/2005.02291.pdf">NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results</a></h3>

				
				<br> <b>[<a href="https://github.com/PruneTruong/GLU-Net">Project</a>] [<a href="https://arxiv.org/abs/1912.05524">Paper</a>] [<a href="https://github.com/PruneTruong/GLU-Net">Code</a>] </b>
	</div></div>


	<div class="publication">
		      <img src="vqm_ntire2020_new.gif" class="item-image">

		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/2005.02291.pdf">
		   NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results
		    </a>
                </strong>
                <br>
                <br>
                Dario Fuoli, <b>Zhiwu Huang</b>, Martin Danelljan, Radu Timofte and et al. 
					
				<br>
				<em> Computer Vision and Pattern Recognition (CVPR) workshop, 2020 </em>   
			    <br>	
			    <span class="links">
		            <a href="https://arxiv.org/pdf/2005.02291.pdf">PDF</a> 

		            </span>	
              </p>
        </div>


	<div class="publication">
		       <img src="vid3oc_intVID.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="http://www.vision.ee.ethz.ch/~timofter/publications/Kim-ICCVW-2019.pdf">
		   The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping
		    </a>
                </strong>
                <br>
                <br>
                Sohyeong Kim, Guanju Li, Dario Fuoli, Martin Danelljan, <b>Zhiwu Huang</b>, Shuhang Gu, Radu Timofte. 
					
				<br>
				<em>  International Conference on Computer Vision (ICCV) workshop, 2019 </em>   
			    <br>	
			    <span class="links">
		            <a href="http://www.vision.ee.ethz.ch/~timofter/publications/Kim-ICCVW-2019.pdf">PDF</a> 
					<!--<a href="https://github.com/musikisomorphie/swd" class="first">GitHub code</a>-->

		            </span>	
              </p>
        </div>


	<div class="publication">
		       <img src="swgm.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1706.02631.pdf">Sliced Wasserstein Generative Models</a>
                </strong>
                <br>
                <br>
                Jiqing Wu*, <b>Zhiwu Huang*</b>, Dinesh Acharya, Wen Li, Janine Thoma, Danda Pani Paudel,  Luc Van Gool. 
				<br>
				<em> (*indicates equal contributions) </em>                
			    <br>	
				<br>
				<em> Computer Vision and Pattern Recognition (CVPR), 2019 </em>   
			    <br>	
			    <span class="links">
		            <a href="https://arxiv.org/abs/1706.02631">arXiv</a> 
					<a href="https://github.com/musikisomorphie/swd" class="first">GitHub code</a>

				 </span>	
              </p>
        </div>


		<div class="publication">
		       <img src="manifoldwgan.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1712.01551.pdf"> Manifold-valued Image Generation with Wasserstein Generative Adversarial Nets </a>
                </strong>
                <br>
                <br>
                 <b>Zhiwu Huang</b>, Jiqing Wu, Luc Van Gool.  
				<br>
				
				<em> Association for the Advancement of Artificial Intelligence (AAAI),  2019 </em>                    
			    <br>
				<span class="links">  <a href="https://arxiv.org/abs/1712.01551"> arXiv </a> </em> 	
              		</p>
        	</div>
		
		<div class="publication">
		       <img src="wgan-div.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1712.01026.pdf"> Wasserstein Divergence for GANs </a>
                </strong>
                <br>
                <br>
                 Jiqing Wu, <b>Zhiwu Huang</b>, Janine Thoma, Dinesh Acharya, Luc Van Gool.  
				<br>
				<em> European Conference on Computer Vision (ECCV), 2018 </em>                
			    <br>	
				<span class="links">
		            <a href="https://arxiv.org/abs/1712.01026">arXiv</a>
					<a href="https://github.com/musikisomorphie/wgan-div" class="first">GitHub code</a>
				 </span>	
              </p>
        </div>
		
		<div class="publication">
		       <img src="cov-pooling.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1805.04855.pdf">Covariance Pooling for Facial Expression Recognition</a>
                </strong>
                <br>
                <br>
                Dinesh Acharya, <b>Zhiwu Huang</b>, Danda Pani Paudel, Luc Van Gool.  
				<br>
				<em> The workshop DiffCVML in CVPR,  2018 </em>                
			    <br>	
				<span class="links">
		            <a href="https://arxiv.org/abs/1805.04855">arXiv</a>
					<a href="https://github.com/d-acharya/CovPoolFER" class="first">GitHub code</a>
				 </span>
              </p>
        </div>
		
		
		<div class="publication">
		       <img src="grnet-structure.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="https://arxiv.org/pdf/1611.05742.pdf">Building Deep Networks on Grassmann Manifolds</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Jiqing Wu,  Luc Van Gool.  
				<br>
				<em> Association for the Advancement of Artificial Intelligence (AAAI),  2018 </em>                
			    <br>	
				<span class="links">
		            <a href="https://arxiv.org/pdf/1611.05742.pdf">arXiv</a>
					<a href="https://github.com/zzhiwu/GrNet" class="first">GitHub code</a>
					<!--<a>GitHub code (coming)</a>-->

					<a href="codes/AFEW_Gr_data.zip" class="first">AFEW Grassmannian data</a>
				 </span>
              </p>
        </div>
		
		<div class="publication">
		       <img src="lienet-structure.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Deep_Learning_on_CVPR_2017_paper.pdf">Deep Learning on Lie Groups for Skeleton-based Action Recognition</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Chengde Wan, Thomas Probst, Luc Van Gool.  
				<br>
				 <em> Computer Vision and Pattern Recognition (CVPR), 2017 <font color="#e86e14">(Spotlight)</font> </em>                
			    <br>
				<!-- <em>  <a href="https://arxiv.org/abs/1612.05877">arXiv:1612.05877</a>, 2016 </em>                
			    <br>	-->
				 <span class="links">
					<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Deep_Learning_on_CVPR_2017_paper.pdf">PDF</a></a>
		            <a href="https://arxiv.org/abs/1612.05877">arXiv</a>
					<a href="https://github.com/zzhiwu/LieNet" class="first">GitHub code</a>
					<!--<a>GitHub code (coming)</a>-->
					<a href="codes/G3D_Lie_data.zip" class="first">G3D LieGroup data</a>
					
				 </span>
              </p>
        </div>
		
		<div class="publication">
		        <img src="spdnet-structure.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <!--  <a href="papers/spdnet-crc.pdf"> A Riemannian Network for SPD Matrix Learning </a>-->
					<a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14633/14371"> A Riemannian Network for SPD Matrix Learning </a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b> and Luc Van Gool. 
				<br>
                <em> Association for the Advancement of Artificial Intelligence (AAAI), 2017 </em>                
			    <br>
				 <span class="links">
		            <a href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewPDFInterstitial/14633/14371">PDF</a></a>
					<a href="https://arxiv.org/abs/1608.04233">arXiv</a></a>
					<!--  <a href="codes/SPDNet-v1.0.zip" class="first">Code</a>-->
					<a href="https://github.com/zzhiwu/SPDNet-master" class="first">GitHub code</a>
					<a href="codes/AFEW_SPD_data.zip" class="first">AFEW  SPD data</a>

				 </span>	
              </p>	
		 </div>
		<div class="container">
	    </div>


        <div class="publication">
            <img src="leml-icml15.png" class="publogo" width = "200 px" >
            <p> 
                <strong>
                    <a href="papers/Huang_LogEuclidean_Metric_Learning_2015_ICML_paper.pdf">Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Ruiping Wang, Shiguang Shan, Xianqiu Li, Xilin Chen.
                <br>
                <em> International Conference on Machine Learning (ICML), 2015 <font color="#e86e14">(Oral)</font></em>
                <br>
                <span class="links">
                    <a href="papers/Huang_LogEuclidean_Metric_Learning_2015_ICML_paper.pdf">PDF</a>
                    <a href="slides/leml_icml2015_slides_v2.0.pdf">Slides</a>
                    <a href="posters/leml_icml2015_poster_v2.0.pdf">Poster</a>
					<a href="codes/LEML-v1.0.zip" class="first">Code</a>
                    <!--  <a href="https://github.com/n-zhang/part-based-RCNN">Code</a> <font color="#e86e14">(Oral)</font>--> 
                </span>
            </p>
        </div>
        <br>
        <br>
        <div class="publication">
        	<img src="pml-cvpr15.png" class="publogo" width = "200 px" >
        	<p> 
        		<strong>
        			<a href="papers/Huang_Projection_Metric_Learning_2015_CVPR_paper_2.pdf">Projection Metric Learning on Grassmann Manifold with Application to Video based Face Recognition</a>
        		</strong>
        		<br>
                <br>
        		<b>Zhiwu Huang</b>, Ruiping Wang, Shiguang Shan, Xilin Chen.
        		<br>
        		<em>Computer Vision and Pattern Recognition (CVPR), 2015</em>
        		<br>
        		<span class="links">
        			<a href="papers/Huang_Projection_Metric_Learning_2015_CVPR_paper_2.pdf">PDF</a>
        			<a href="posters/PML_CVPR15_Poster_v1.0.pdf">Poster</a>   
                    <a href="codes/PML-v1.0.zip" class="first">Code</a>					
        		</span>
        	</p>
        </div>
        <br>
        <br>
        <br>
		<div class="publication">
        	<img src="her-cvpr15.png" class="publogo" width="200 px">
        	<p> 
        		<strong>
        			<a href="papers/Li_Face_Video_Retrieval_2015_CVPR_paper.pdf">Face Video Retrieval with Image Query via Hashing across Euclidean Space and Riemannian Manifold</a>
        		</strong>
        		<br>
                 <br>
        		Yan Li, Ruiping Wang, <b>Zhiwu Huang</b>, Shiguang Shan, Xilin Chen.
        		<br>
        		<em>Computer Vision and Pattern Recognition (CVPR), 2015</em>
        		<br>
        		<span class="links">
        			<a href="papers/Li_Face_Video_Retrieval_2015_CVPR_paper.pdf">PDF</a>
                    <a href="http://vipl.ict.ac.cn/resources/codes/code/HER_v1.0.rar" class="first">Code</a>
        		</span>
        	</p>
        </div>
        <br>
        <br>
        <br>
        <div class="publication">
        	<img src="darg-cvpr15.png" width="200 px"class="publogo">
        	<p> 
        		<strong>
        			<a href="papers/Wang_Discriminant_Analysis_on_2015_CVPR_paper.pdf">Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets</a>
        		</strong>
        		<br>
                 <br>
        		Wen Wang, Ruiping Wang, <b>Zhiwu Huang</b>, Shiguang Shan, Xilin Chen.
        		<br>
        		<em>Computer Vision and Pattern Recognition (CVPR), 2015</em>
        		<br>
        		<span class="links">
        			<a href="papers/Wang_Discriminant_Analysis_on_2015_CVPR_paper.pdf">PDF</a> 
					<a href="http://vipl.ict.ac.cn/resources/codes/code/DARG-Release-v1.0.zip" class="first">Code</a>
  					
        		</span>

        	</p>
        </div>      
        <br>
        <br>
        <br>
        <div class="publication">
        	<img src="lerm-cvpr14.png" width = "200 px" class="publogo">
        	<p> 
        		<strong>
        			<a href="papers/lerm_cvpr14_5118b677.pdf">Learning Euclidean-to-Riemannian Metric for Point-to-Set Classification</a>
        		</strong>
        		<br>
                <br>
        		<b>Zhiwu Huang</b>, Ruiping Wang, Shiguang Shan, Xilin Chen. 
        		<br>
        		<em>Computer Vision and Pattern Recognition (CVPR), 2014 <font color="#e86e14">(Oral)</font> </em>
        		<br>
        		<span class="links">
        			<a href="papers/lerm_cvpr14_5118b677.pdf">PDF</a>
					<a href="papers/sup_for_lerm_cvpr14_886_final.pdf">SUP</a>
					<a href="slides/LERM_CVPR14_Slides_v4.0.pdf">Slides</a>
					<a href="codes/LERM-v1.0.rar" class="first">Code</a>
					<!--  <a href="codes/PML-v1.0.zip" class="first">Code</a> -->

        		</span>
        	</p>
        </div>
        <br>
        <br>
        <div class="publication">
        	<img src="car-iccv13.png" width = "200 px" class="publogo">
        	<p> 
        		<strong>
        			<a href="papers/2013_ICCV_zwhuang_CAR.pdf">Coupling Alignments with Recognition for Still-to-Video Face Recognition </a>
        		</strong>
        		<br>
                <br>
        		<b>Zhiwu Huang</b>, Xiaowei Zhao, Shiguang Shan, Ruiping Wang, Xilin Chen. 
        		<br>
        		<em>International Conference on Computer Vision (ICCV), 2013 </em>
        		<br>
        		<span class="links">
        			<a href="papers/2013_ICCV_zwhuang_CAR.pdf">PDF</a>
					<a href="posters/ICCV13_Poster_ZhiwuHuang_v2.0.pdf">Poster</a>
					<!--  <a href="http://vipl.ict.ac.cn/sites/default/files/people/attach/CAR-v1.0.rar">Code</a>-->
					<a href="codes/CAR-v1.0.rar" class="first">Code</a>
					<a href="demos/CAR_ICCV13_Demo.avi">Demo</a>
					
        		</span>
        	</p>
        </div>
        <br>       
    </div>
	
	<div class="container">
		<h2>Selected Journal Papers</h2>
			<div class="publication">
				   <img src="cerml_overview.png" class="publogo" width = "200 px" >
				   <p> 
					<strong>
						<a href="http://arxiv.org/pdf/1608.04200.pdf">Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video</a>
					</strong>
					<br>
					<br>
					<b>Zhiwu Huang</b>, Ruiping Wang, Shiguang Shan,  Luc Van Gool, Xilin Chen.  
					<br>
					<em>  IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018 </em>                
					<br>	
					 <span class="links">
						<a href="http://arxiv.org/pdf/1608.04200.pdf">arXiv</a>
					 </span>
				  </p>
			</div>	
			
			 <br>
             <br>
			 
			 <div class="publication">
				   <img src="darg-tip.png" class="publogo" width = "200 px" >
				   <p> 
					<strong>
						<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022922">Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets</a>
					</strong>
					<br>
					<br>
					Wen Wang, Ruiping Wang, <b>Zhiwu Huang</b>, Shiguang Shan, Xilin Chen.  
					<br>
					<em>  IEEE Transactions on Image Processing (TIP), 2018 </em>                
					<br>	
					 <span class="links">
						<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022922">PDF</a>
					 </span>
				  </p>
			</div>	
			
			 <br>
             <br>
			 
			
			<div class="publication">
		       <img src="spd_schemes.png" class="publogo" width = "200 px" >
		       <p> 
		        <strong>
                    <a href="http://arxiv.org/pdf/1608.04914.pdf">Geometry-aware Similarity Learning on SPD Manifolds for Visual Recognition</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc Van Gool, Xilin Chen.  
				<br>
				<em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2017</em>
                <br>
				<em>  <a href="http://arxiv.org/abs/1608.04914"> arXiv</a> </em>                
			    <br>	
				<!--  <span class="links">
		            <a href="http://arxiv.org/pdf/1608.04914.pdf">PDF</a>
				 </span>-->	
              </p>
        </div>
		<br>
        <br>
		
		<div class="publication">
            <img src="cox-example-tip15.png" class="publogo" width = "200 px" >
            <p> 
                <strong>
                    <a href="papers/COX-Face-DB-TIP-final.pdf">A Benchmark and Comparative Study of Video-based Face Recognition on COX Face Database</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Shiguang Shan, Ruiping Wang, Haihong Zhang, Shihong Lao, Alifu Kuerban, Xilin Chen.
                <br>
                <em>IEEE Transactions on Image Processing (TIP), 2015</em>
                <br>
                <span class="links">
		            <a href="papers/COX-Face-DB-TIP-final.pdf">PDF</a>                    
                    <!--<a href="http://vipl.ict.ac.cn/resources/datasets/cox-face-dataset/main">Project page</a>-->
                </span>
            </p>
        </div>
        <br>
        <br>
		
		 <div class="publication">
            <img src="herml-framework-pr15.png" class="publogo" width = "200 px" >
            <p> 
                <strong>
                    <a href="papers/PR-HERML-final.pdf">Face Recognition on Large-scale Video in the Wild with Hybrid Euclidean-and-Riemannian Metric Learning</a>
                </strong>
                <br>
                <br>
                <b>Zhiwu Huang</b>, Ruiping Wang, Shiguang Shan, Xilin Chen.
                <br>
                <em>Pattern Recognition (PR), 2015</em>
                <br>
                <span class="links">
                    <a href="papers/PR-HERML-final.pdf">PDF</a>
                </span>
            </p>
        </div>
        <br>
        <br>
		
	 </div>


	<h2>Research</h2>

Selected research projects and papers.


<h3><b>CVPR 2020:</b> <a href="https://github.com/PruneTruong/GLU-Net">GLU-Net: Global-Local Universal Network for Dense Flow and Correspondences</a></h3>
<div class="image-txt-container">
<img src="glunetfig.jpg" class="item-image">
	<div>
In this work, we propose a universal network architecture for establishing dense correspondences between two images. The same network achieves state-of-the-art performance for geometric and semantic matching as well as optical flow. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution.
		<br> <b>[<a href="https://github.com/PruneTruong/GLU-Net">Project</a>] [<a href="https://arxiv.org/abs/1912.05524">Paper</a>] [<a href="https://github.com/PruneTruong/GLU-Net">Code</a>] </b>
</div></div>


<h3><b>CVPR 2020:</b> <a href="https://github.com/visionml/pytracking">Probabilistic Regression for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="prdimpfig.png" class="item-image">
	<div>
This work proposes a general formulation for probabilistic regression, which is then applied to visual tracking. Our network predicts the conditional probability density of the target state given an input image. The probability density is flexibly parametrized by the neural network itself. Crucially, our formulation is capable of modeling label noise stemming from inaccurate annotations and ambiguities in the task. The regression network is trained by directly minimizing the Kullback-Leibler divergence. 
		<br> <b>[<a href="https://github.com/visionml/pytracking">Project</a>] [<a href="https://arxiv.org/abs/2003.12565">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>


<h3><b>CVPR 2020:</b> <a href="https://github.com/andr345/frtm-vos">Learning Fast and Robust Target Models for Video Object Segmentation</a></h3>
<div class="image-txt-container">
<img src="frtmfig.jpg" class="item-image">
	<div>
This work proposes a novel architecture for Video Object Segmentation (VOS). It integrates a target appearance model, which consists of a light-weight module that is learned during the inference stage. To this end, we employ fast optimization techniques in order to learn to predict a coarse but robust target segmentation. The coarse mask is processed by a segmentation network, that is exclusively trained offline to predict a high quality segmentation. Our method is fast, easily trainable and remains highly effective in cases of limited training data. 
		<br> <b>[<a href="https://github.com/andr345/frtm-vos">Project</a>] [<a href="https://arxiv.org/abs/2003.00908">Paper</a>] [<a href="https://github.com/andr345/frtm-vos">Code</a>] </b>
</div></div>


<h3><b>CVPR 2020:</b> <a href="https://github.com/vaesl/IP-Net">Learning Human-Object Interaction Detection using Interaction Points</a></h3>
<div class="image-txt-container">
<img src="hoifig.jpg" class="item-image">
	<div>
Human-object interaction (HOI) detection strives to localize both the human and an object as well as the identification of complex interactions between them.  Most existing HOI detection approaches are instance-centric where interactions between all possible human-object pairs are predicted based on appearance features and coarse spatial information. We argue that appearance features alone are insufficient to capture complex human-object interactions. In this paper, we therefore propose a novel fully-convolutional approach that directly detects the interactions between human-object pairs. Our network predicts interaction points, which directly localize and classify the interaction. Paired with the densely predicted interaction vectors, the interactions are associated with human and object detections to obtain final predictions.
		<br> <b>[<a href="https://github.com/vaesl/IP-Net">Project</a>] [<a href="https://arxiv.org/abs/2003.14023">Paper</a>] [<a href="https://github.com/vaesl/IP-Net">Code</a>] </b>
</div></div>


<h3><b>ICCV 2019:</b> <a href="https://visionml.github.io/dimp/">Learning Discriminative Model Prediction for Tracking</a></h3>
<div class="image-txt-container">
<img src="dimpfig.png" class="item-image">
	<div>
In this work, we develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, while running at over 40 FPS.
		<br> <b>[<a href="https://visionml.github.io/dimp/">Project</a>] [<a href="https://arxiv.org/abs/1904.07220">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>

<h3><b>ICCV 2019:</b> <a href="https://arxiv.org/abs/1908.00855">Learning the Model Update for Siamese Trackers.</a></h3>
<div class="image-txt-container">
<img src="updatenet.png" class="item-image">
	<div>
We propose to replace the handcrafted update function in Siamese trackers with a learnable update mechanism. We use a convolutional neural network, called UpdateNet, which given the initial template, the accumulated template and the template of the current frame estimates the optimal template for the next frame.
		<br> <b>[<a href="https://arxiv.org/abs/1908.00855">Paper</a>] [<a href="https://github.com/zhanglichao/updatenet">Code</a>] </b>
</div></div>

<h3><b>CVPR 2019:</b> <a href="https://visionml.github.io/atom/">ATOM: Accurate Tracking by Overlap Maximization</a></h3>
<div class="image-txt-container">
<img src="atom_overview.png" class="item-image">
	<div>
In this work we primarily address the problem of performing accurate bounding box estimation for generic visual tracking. We train a target estimation module offline, conditioned on the target appearance, to predict the overlap between the object and a bounding box estimate. Furthermore, we propose a target classification component that is learned online using dedicated optimization techniques.
		<br> <b>[<a href="https://visionml.github.io/atom/">Project</a>] [<a href="https://arxiv.org/abs/1811.07628">Paper</a>] [<a href="https://github.com/visionml/pytracking">Code</a>] </b>
</div></div>

<h3><b>CVPR 2019:</b> <a href="https://arxiv.org/abs/1811.11611">A Generative Appearance Model for End-to-end Video Object Segmentation</a></h3>
<div class="image-txt-container">
<img src="AGAMEfig.png" class="item-image">
	<div>
		Here, we propose a fully end-to-end architecture for Video Object Segmentation. We introduce a generative appearance module that generates class-conditional probabilities. This provides a highly discriminative cue, which is processed in later network modules.
		<br> <b>[<a href="https://arxiv.org/abs/1811.11611">Paper</a>] [<a href="https://github.com/joakimjohnander/agame-vos">Code</a>] </b>
</div></div>

<h3><b>ECCV 2018:</b> <a href="https://arxiv.org/abs/1804.06833">Unveiling the Power of Deep Tracking</a></h3>
<div class="image-txt-container">
<img src="UPDTfig.png" class="item-image">
	<div>
In this work, we investigate the issue of utilizing deep features for tracking. We systematically study the characteristics of both deep and shallow features, and their relation to tracking accuracy and robustness. We identify the limited data and low spatial resolution as the main challenges, and propose strategies to counter these issues when integrating deep features for tracking. Furthermore, we propose a novel adaptive fusion approach that leverages the complementary properties of deep and shallow features to improve both robustness and accuracy.
		<br> <b> [<a href="https://arxiv.org/abs/1804.06833">Paper</a>] </b>
</div></div>

<h3><b>CVPR 2018:</b> <a href="https://arxiv.org/abs/1804.01495">Density Adaptive Point Set Registration</a></h3>
<div class="image-txt-container">
<img src="DAREfig.png" class="item-image">
	<div>
		We revisit the foundations of the probabilistic point cloud registration paradigm, in order to tackle the key issue of sampling density variations. Contrary to previous works, we model the underlying structure of the scene as a latent probability distribution, and thereby induce invariance to point set density changes. Both the probabilistic model of the scene and the registration parameters are inferred by minimizing the Kullback-Leibler divergence in an Expectation Maximization based framework.
		<br> <b>[<a href="https://arxiv.org/abs/1804.01495">Paper</a>] [<a href="https://github.com/felja633/DARE">Code</a>] </b>
</div></div>

<h3><b>CVPR 2017:</b> <a href="https://visionml.github.io/eco/">ECO: Efficient Convolution Operators for Tracking</a></h3>
<div class="image-txt-container">
<img src="ECOfig.png" class="item-image">
<div>In this work we tackle the key causes behind the problems of computational complexity <i>and</i> over-fitting in advanced DCF trackers. We revisit the core DCF formulation and introduce: (i) a factorized convolution operator, which drastically reduces the number of parameters in the model; (ii) a compact generative model of the training sample distribution, that significantly reduces memory and time complexity, while providing better diversity of samples; (iii) a conservative model update strategy with improved robustness and reduced complexity.
	<br><b>[<a href="https://visionml.github.io/eco/">Project</a>] [<a href="https://arxiv.org/abs/1611.09224">Paper</a>] [<a href="https://github.com/martin-danelljan/ECO">Code</a>]</b>
</div>
</div>	

	<h3><b>ECCV 2016:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html">Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="CCOTfig.png" class="item-image">
	<div>In this work we develop a theoretical framework for discriminatively learning a convolution operator in the continuous spatial domain. Our formulation enables a natural integration of multi-resolution deep feature maps. In addition, our continuous formulation is capable of accurate sub-pixel localization of the target.
		<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/index.html">Project</a>] [<a href="https://arxiv.org/abs/1608.03773">Paper</a>] [<a href="https://github.com/martin-danelljan/Continuous-ConvOp">Code</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2016:</b> <a href="http://www.cvl.isy.liu.se/research/cogvis/colored-point-set-registration/index.html">A Probabilistic Framework for Color-Based Point Set Registration</a></h3>
<div class="image-txt-container">
<img src="ColorPCregFig.png" class="item-image">
	<div>In work we propose a probabilistic point set registration framework that exploits available <b>color</b> information associated with the points. Our method is based on a model of the joint distribution of 3D-point observations and their color information. We derive an EM algorithm for jointly estimating the model parameters and the relative transformations. The proposed model captures discriminative color information, while being computationally efficient.
		<br><b>[<a href="http://www.cvl.isy.liu.se/research/cogvis/colored-point-set-registration/index.html">Project</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Danelljan_A_Probabilistic_Framework_CVPR_2016_paper.pdf">Paper</a>] [<a href="https://github.com/felja633/DARE">Code</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2016:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html">Adaptive Decontamination of the Training Set: A Unified Formulation for Discriminative Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="DeconFig.png" class="item-image">
	<div>In this work we propose a unified formulation for alleviating the problem of corrupted training samples in tracking-by-detection methods. This is achieved by minimizing a joint loss over both target appearance model and the training sample quality weights. Our approach is generic and can be integrated into any discriminative tracking framework.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/decontrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1609.06118">Paper</a>]</b>
</div>
</div>
	
	<h3><b>ICCV 2015:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html">Learning Spatially Regularized Correlation Filters for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="SRDCFfig_small.png" class="item-image">
	<div>In this work we propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. This effectively mitigates the unwanted boundary effects, which limits the performance of standard correlation based trackers. The SRDCF tracker <b>won</b> the recent OpenCV Challenge and achieved the <b>best result</b> in the VOT-TIR2015 challenge.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/regvistrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1608.05571">Paper</a>]</b>
</div>
</div>
	
	<h3><b>TPAMI, BMVC 2014:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html">Scale Estimation for Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="DSSTpyramidFig_small.png" class="item-image">
	<div>Here we investigate the problem of accurate and fast scale estimation for visual tracking. The proposed Discriminative Scale Space Tracker (DSST) <b>won</b> the Visual Object Tracking (VOT) 2014 challenge.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/scalvistrack/index.html">Project and Code</a>] [<a href="https://arxiv.org/abs/1609.06141">Paper, PAMI</a>] [<a href="http://www.bmva.org/bmvc/2014/files/paper038.pdf">Paper, BMVC</a>]</b>
</div>
</div>
	
	<h3><b>CVPR 2014:</b> <a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html">Adaptive Color Attributes for Real-Time Visual Tracking</a></h3>
<div class="image-txt-container">
<img src="CNfig_small.png" class="item-image">
	<div>In this work, we investigated how to incorporate color information into visual tracking.
	<br><b>[<a href="http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html">Project and Code</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2014/papers/Danelljan_Adaptive_Color_Attributes_2014_CVPR_paper.pdf">Paper</a>]</b>
</div>
</div>

<h2>Publications</h2>
See my <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=en">Google Scholar profile</a> or my <a href="http://dblp.uni-trier.de/pers/hd/d/Danelljan:Martin">dblp page</a> for a list of publications.

</body>

