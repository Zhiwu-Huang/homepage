<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">

  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #07889b; /*#1772d0;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #e37222; /*#f7b733;*/ /*f09228;*/
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    strong {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    }
    heading {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 22px;
    color: #e37222; /*#fc4a1a;*/
    }
    heading2 {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 18px;
    }
    papertitle {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 15px; /*14*/
    font-weight: 700;
    }
    name {
    font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif; /*'Lato', Verdana, Helvetica, sans-serif;*/
    font-size: 42px;
    }
    li:not(:last-child) {
        margin-bottom: 5px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>

  <link rel="icon" type="image/png" href="https://people.eecs.berkeley.edu/~barron/seal_icon.png">
  <title>Zhiwu Huang, ETH Zurich</title>

  <link href="./_files/css" rel="stylesheet" type="text/css">
  <style id="dark-reader-style" type="text/css">@media screen {

/* Leading rule */
/*html {
  -webkit-filter: brightness(100%) contrast(100%) grayscale(20%) sepia(10%) !important;
}*/

/* Text contrast */
html {
  text-shadow: 0 0 0 !important;
}

/* Full screen */
*:-webkit-full-screen, *:-webkit-full-screen * {
  -webkit-filter: none !important;
}

/* Page background */
html {
  background: rgb(255,255,255) !important;
}

}</style>

<script type="text/javascript">
   function visibility_on(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'none')
            e.style.display = 'block';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'none')
            e.style.display = 'block';
   }
   function visibility_off(id) {
        var e = document.getElementById(id+"_text");
        if(e.style.display == 'block')
            e.style.display = 'none';
        var e = document.getElementById(id+"_img");
        if(e.style.display == 'block')
            e.style.display = 'none';
   }
   function toggle_visibility(id) {
       var e = document.getElementById(id+"_text");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
       var e = document.getElementById(id+"_img");
       if(e.style.display == 'inline')
          e.style.display = 'block';
       else
          e.style.display = 'inline';
   }
   function toggle_vis(id) {
       var e = document.getElementById(id);
       if (e.style.display == 'none')
           e.style.display = 'inline';
       else
           e.style.display = 'none';
   }
</script>

</head>
</div>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="center">
        <name>Zhiwu Huang</name><br>
        zhiwu.huang at vision dot ee dot ethz doc ch
        </p>
        <p>I am a postdoc under the supervision of Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, ETH Zurich. My research interest lies in Computer Vision and Machine Learning for <b> Automated Video Artificial Intelligence, </b> capable of autmatically learning to understand the world through videos. I currently working on human-focussed video clustering, classification, prediction as well as video generation, enhancement and manipulation with deep manifold learning, generative distribution learning, and neural architecture learning.<br><br>
        </p>
          
       <p> In the lab, I am mainly working for the following research projects: </p>
			
			<p> <b> Video Generation and Classification </b> with <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel, </a> <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu, </a> </a> <a href="https://scholar.google.ch/citations?user=pfEoUpcAAAAJ&hl=de"> Thomas Probst </a>  </p>
			<p> <b> Video Enhancement </b> with <a href="https://scholar.google.ch/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte, </a> <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel, </a> <a href="http://www.vision.ee.ethz.ch/en/members/detail/408/"> Dario Fuoli, </a> <a href="https://scholar.google.com/citations?hl=en&user=NCSSpMkAAAAJ"> Martin Danelljan </a>   </p>

			<p> <b> Video Manipulation </b> with <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel, </a> <a href="https://scholar.google.ch/citations?user=3BHMHU4AAAAJ&hl=en&oi=ao"> Ajad Chhatkuli, </a> <a href="https://vision.ee.ethz.ch/people-details.MjY0NDM1.TGlzdC8zMjg5LC0xOTcxNDY1MTc4.html/"> Mohamad Shahbazi </a>  </p>

			<p> <b> Video Clustering </b> with <a href="https://scholar.google.ch/citations?hl=en&user=wbk0QAcAAAAJ"> Suryansh Kumar </a> </p>
        
        <p>
Before coming to Zurich, I obtained my PhD degree from Visual Information Processing and Learning (VIPL) group in Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS), in 2015. Prof. <a href="https://scholar.google.com/citations?user=Vkzd7MIAAAAJ&hl=en">Shiguang Shan</a> is my PhD supervisor. During my PhD study, Prof. <a href="https://scholar.google.com/citations?user=duIUwpwAAAAJ">Ruiping Wang</a> worked as my mentor, and I also worked closely with Prof. <a href="https://scholar.google.com/citations?user=vVx2v20AAAAJ">Xilin Chen</a>.        </p>

       <!-- <b>Prospective students and post-docs</b>, please <a href="javascript:toggle_vis('contact')">read this</a>  before contacting me.
              <div id="contact" style="display:none"> 
                  Thank you for your interest in joining my lab! I am taking on new MS and PhD students each year. However, I ask that you do not contact me directly with regard to MS or PhD admissions until after you are admitted, as I will not be able to reply to individual emails. <br>
                  If you are interested in a <i>post-doc</i> position, please read <a href="https://goo.gl/forms/aKL2gnq8T80FNVMb2">this form</a>. <br>
                  If you are a current or admitted <i>Stanford undergraduate or MS student</i> interested in research positions, please read <a href="https://goo.gl/forms/uJuYOfIBQVPhEEDy1">this form</a>. <br>
                  If you are not a Stanford student and insteresed in research positions, please read <a href="https://goo.gl/forms/9scJRH3hw6z7GNbj2">this form</a>.
              </div>
        </p>-->


        <p align="center">
            <a href="cv.pdf">CV</a> &nbsp;/&nbsp;
	    <a href="https://vision.ee.ethz.ch/people-details.MjIwNDU5.TGlzdC8zMjkyLC0xOTcxNDY1MTc4.html">Contract Details </a> &nbsp;/&nbsp;
            <a href="http://scholar.google.ch/citations?user=yh6t92AAAAAJ&hl=en">Google Scholar</a> 
        </p>
        </td>
        <td width="33%">
        <img height="280" width="280" src="zhiwu.png">
        </td>
      </tr>
  </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
            <heading>News</heading>
            <ul>
                <li> 07/2020, One paper "Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search" is accepted by ECCV 2020. </li>
	 	<li> 05/2020, We are organizing <a href= "https://competitions.codalab.org/competitions/24685"> AIM Video Super-Resolution Challenge </a> @ECCV 2020. </li>
 		<li> 05/2020, One paper "NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results" will appear at the workshop NTIRE in conjunction with CVPR 2020. </li>
    		<li> 12/2019, We are organizing <a href= "https://competitions.codalab.org/competitions/20247"> NTIRE Video Quality Mapping Challenge </a> @CVPR 2020. </li>
    		<li> 10/2019, One dataset paper "The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping" will appear at the workshop AIM in ICCV 2019. </li>
		<li>10/2019, We are organizing a workshop <a href= "http://www.vision.ee.ethz.ch/aim19/"> "AIM: Advances in Image Manipulation workshop and challenges on image and video manipulation" </a>
  and a tutorial <a href= "http://www.vision.ee.ethz.ch/fire19/index_fire19.html"> "FIRE: From Image Restoration to Enhancement and Beyond" </a> at ICCV, Oct. 27, 2019. </li>

		<li>02/2019, One paper "Sliced Wasserstein Generative Models" is accepted by CVPR 2019. This paper was selected as one of the BEST PUBLICATIONS of the week (20.04.2019), by DeepAI. <a  href="https://deepai.org/publication/sliced-wasserstein-generative-models"> Link to DeepAI Website </a> </li>

		<li> <a href="javascript:toggle_vis('news')"> Older news...</a> </li>
		<div id="news" style="display:none"> 
			<li>11/2018, One paper "Manifold-valued Image Generation with Wasserstein Generative Adversarial Nets" is accepted by AAAI 2019. <em> This paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli (2018 Fields Medal winner) to generative modeling, which is able to generate photo-realistic biological samples. </em> </li>
	<!--<b> The paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli, who won 2018 Fields Medal. </b> -->
			<li>07/2018, One paper "Wasserstein Divergence for GANs" is accepted by ECCV 2018.</li>
			<li>04/2018, One paper "Covariance Pooling for Facial Expression Recognition" is accepted by the workshop DiffCVML in CVPR 2018.</li>
			<li>11/2017, One paper "Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video" is accepted as a Regular Paper in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</li>
			<li>11/2017, One paper "Building Deep Networks on Grassmann Manifolds" (GrNet) is accepted by AAAI 2018.</li>
		    <li>08/2017, One paper "Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets" is accepted by IEEE Transactions on Image Processing (TIP).</li>
			<li>07/2017, One paper "Geometry-aware Similarity Learning on SPD Manifolds for Visual Recognition" is accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
			<li>02/2017, One paper "Deep Learning on Lie Groups for Skeleton-based Action Recognition" (LieNet) is accepted as a spotlight by CVPR 2017. <em> This paper proposes deep networks of Lie Groups for skeleton-based action recognition. </em> </li>
		    <li>11/2016, One paper "A Riemannian Network for SPD Matrix Learning" (SPDNet) is accepted by AAAI 2017. <em>  This paper opens a new direction of deep manifold networks. </em> </li>
	     	</div>	
            </ul>
        </td></tr>



        <td width="100%" valign="middle">
            <heading>Publications</heading> <br><br>

          <heading2><i>Preprints</i></heading2><br><br>

            <div onmouseover="document.getElementById('pcgrad').style.display = 'block';"
                onmouseout="document.getElementById('pcgrad').style.display='none';">
              <a href="https://arxiv.org/pdf/2001.06782.pdf">
                <papertitle>Gradient Surgery for Multi-Task Learning</papertitle></a><br>
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>,
              <a href="https://scholar.google.com/citations?user=Rkr2uT8AAAAJ&hl=en/">Saurabh Kumar</a>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://karolhausman.github.io/">Karol Hausman</a>,
              <i>Chelsea Finn</i><br>
              <!--<em>Conference on Robot Learning (CoRL)</em>, 2019 <br>-->
              <a href="https://arxiv.org/abs/2001.06782">arXiv</a>  / <a href="https://github.com/tianheyu927/PCGrad">code</a>
            </div>
            <div id="pcgrad" style="display:none">  
              </div><br>


            <div onmouseover="document.getElementById('moca').style.display = 'block';"
                onmouseout="document.getElementById('moca').style.display='none';">
              <a href="https://arxiv.org/pdf/1912.08866.pdf">
                <papertitle>Continuous Meta-Learning without Tasks </papertitle></a><br>
              <a href="https://stanford.edu/~jh2/" >James Harrison</a>, 
              <a href="http://asl.stanford.edu/people/apoorva-sharma/">Apoorva Sharma</a>, 
              <i>Chelsea Finn</i>,
              <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a>
              <br>
              <a href="https://arxiv.org/abs/1912.08866">arXiv</a> 
            </div>
              <div id="moca" style="display:none">
              </div><br>


            <div onmouseover="document.getElementById('smirl').style.display = 'block';"
                onmouseout="document.getElementById('smirl').style.display='none';">
              <a href="https://arxiv.org/pdf/1912.05510.pdf">
                <papertitle>SMiRL: Surprise Minimizing RL in Dynamic Environments</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~gberseth/">Glen Berseth</a>, 
              <a href="https://github.com/dangeng">Daniel Geng</a>, 
              <a href="https://people.eecs.berkeley.edu/~coline/">Coline Devin</a>, 
              <i>Chelsea Finn</i>,
              <a href="https://people.eecs.berkeley.edu/~dineshjayaraman/">Dinesh Jayaraman</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <a href="https://arxiv.org/abs/1912.05510">arXiv</a>  / <a href="https://sites.google.com/corp/view/surpriseminimization">videos</a>
            </div>
              <div id="smirl" style="display:none">
              </div><br>

            <div onmouseover="document.getElementById('umrl').style.display = 'block';"
                onmouseout="document.getElementById('umrl').style.display='none';">
              <a href="https://arxiv.org/pdf/1806.04640.pdf">
                <papertitle>Unsupervised Meta-Learning for Reinforcement Learning</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, 
              <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>, 
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
              <a href="https://arxiv.org/abs/1806.04640">arXiv</a> <br>
              <div id="umrl" style="display:none">
              While meta-learning enables fast learning of new tasks, it requires a human to specify a distribution over tasks for meta-training. In effect, meta-learning offloads the design burden from algorithm design to task design. We propose to automate the design of tasks for meta-learning, describing a family of unsupervised meta-reinforcement learning algorithms that are truly automated.
              </div>
            </div><br>

            <heading2><i>2020</i></heading2><br><br>

            <div onmouseover="document.getElementById('poi').style.display = 'block';"
                onmouseout="document.getElementById('poi').style.display='none';">
              <a href="https://arxiv.org/pdf/1912.12773.pdf">
                <papertitle>Learning Predictive Models From Observation and Interaction</papertitle></a><br>
              <a href="https://sites.google.com/corp/view/karlschmeckpeper">Karl Schmeckpeper</a>,
              <a href="https://anxie.github.io/">Annie Xie</a>,
              <a href="https://www.seas.upenn.edu/~oleh/">Oleh Rybkin</a>, 
              <a href="https://s-tian.github.io/">Stephen Tian</a>,
              <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>,
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1912.12773">arXiv</a> / <a href="https://sites.google.com/view/lpmfoai">project page</a>
            </div>
              <div id="poi" style="display:none">
              </div><br>

            <div onmouseover="document.getElementById('esmaml').style.display = 'block';"
                onmouseout="document.getElementById('esmaml').style.display='none';">
              <a href="https://arxiv.org/pdf/2003.01239.pdf">
                <papertitle>Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning</papertitle></a><br>
            <a href="https://research.google/people/XingyouSong/">Xingyou Song</a>, 
              <a href="https://yxyang.github.io/">Yuxiang Yang</a>,
              <a href="https://research.google/people/KrzysztofChoromanski/">Krzysztof Choromanski</a>, 
              <a href="https://ai.google/research/people/KenCaluwaerts/">Ken Caluwaerts</a>, 
              <a href="https://dblp.org/pers/g/Gao:Wenbo.html">Wenbo Gao</a>, 
              <i>Chelsea Finn</i>,
              <a href="http://www.jie-tan.net/">Jie Tan</a>,
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/2003.01239">arXiv</a> / <a href="https://www.youtube.com/watch?v=_QPMCDdFC3E&feature=youtu.be">video</a>
            </div>
              <div id="esmaml" style="display:none">
              </div><br>



            <iv onmouseover="document.getElementById('milli').style.display = 'block';"
                onmouseout="document.getElementById('milli').style.display='none';">
              <a href="https://arxiv.org/pdf/2003.02636">
                <papertitle>Scalable Multi-Task Imitation Learning with Autonomous Improvement</papertitle></a><br>
             <a href="https://people.eecs.berkeley.edu/~avisingh/">Avi Singh</a>,
              <a href="http://evjang.com/">Eric Jang</a>,
               <a href="https://am.is.tuebingen.mpg.de/person/dkappler">Daniel Kappler</a>, 
               <a href="https://cs.stanford.edu/people/khansari/">Mohi Khansari</a>, 
               <a href="https://scholar.google.com/citations?user=M8zPKWgAAAAJ&hl=en">Murtaza Dalal</a>,
               <a href="https://www.alexirpan.com/about/">Alex Irpan</a>,
               <br>
               <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>, 
               <a href="https://cs.stanford.edu/people/khansari/">Mohi Khansari</a>, 
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/2003.02636">arXiv</a>  / <a href="https://sites.google.com/view/scalable-mili">project page</a><br>
              <div id="milli" style="display:none">
              </div>
            </div><br>


            <div onmouseover="document.getElementById('tr').style.display = 'block';"
                onmouseout="document.getElementById('tr').style.display='none';">
              <a href="https://arxiv.org/pdf/1810.01128.pdf">
                <papertitle>Time Reversal as Self-Supervision</papertitle></a><br>
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <a href="http://mb2.web.engr.illinois.edu/">Mohammad Babaeizadeh</a>, 
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://vikashplus.github.io/">Vikash Kumar</a><br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1810.01128">arXiv</a>  / <a href="https://sites.google.com/view/time-reversal">project page</a><br>
              <div id="tr" style="display:none">
               We propose a technique that uses time-reversal to learn goals and provide a high level plan to reach them. In particular, our approach explores outward from a set of goal states and learns to predict these trajectories in reverse, which provides a high-level plan towards goals.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('omni').style.display = 'block';"
                onmouseout="document.getElementById('omni').style.display='none';">
              <a href="https://arxiv.org/pdfd/2003.06965">
                <papertitle>OmniTact: Compact Multi-Directional Optical Tactile Sensor for Robotic Manipulation</papertitle></a><br>
            <a href="https://akhilpadmanabha.wixsite.com/portfolio/about">Akhil Padmanabha</a>,
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              <a href="https://s-tian.github.io/">Stephen Tian</a>,
              <a href="https://www.robertocalandra.com/about/">Roberto Calandra</a>,
              <i>Chelsea Finn</i>,
               <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/2003.06965">arXiv</a>  / <a href="https://sites.google.com/corp/berkeley.edu/omnitact">project page</a><br>
              <div id="omni" style="display:none">
              </div>
            </div><br>



            <div onmouseover="document.getElementById('memo').style.display = 'block';"
                onmouseout="document.getElementById('memo').style.display='none';">
              <a href="https://arxiv.org/pdf/1912.03820.pdf">
                <papertitle>Meta-Learning without Memorization</papertitle></a><br>
              <a href="https://mingzhang-yin.github.io//">Mingzhang Yin</a>, 
              <a href="https://sites.google.com/corp/view/gjt/">George Tucker</a>, 
              <a href="https://mingyuanzhou.github.io//">Mingyuan Zhou</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <font color="#e37222"><strong>(Spotlight)</strong></font> <br>
              <a href="https://arxiv.org/abs/1912.03820">arXiv</a>  / <a href="https://slideslive.com/38921876/bayesian-deep-learning-3">talk</a> / <a href="_files/neurips19_memorization.pdf">slides</a> / <a href="https://github.com/google-research/google-research/tree/master/meta_learning_without_memorization">code</a> 
            </div>
              <div id="memo" style="display:none">
              We identify and formally describe a peculiar, yet widespread problem with meta-learning algorithms that occurs from small and seemingly benign changes to the training set-up, and identify a meta-regularization solution for solving the problem for multiple classes of meta-learning methods.
              </div><br>

            <div onmouseover="document.getElementById('wtl').style.display = 'block';"
                onmouseout="document.getElementById('wtl').style.display='none';">
              <a href="https://arxiv.org/pdf/1906.03352.pdf">
                <papertitle>Watch, Try, Learn: Meta-Learning from Demonstrations and Rewards</papertitle></a><br>
               <a href="http://bland.website/">Allan Zhou</a>,
               <a href="http://evjang.com/">Eric Jang</a>,
               <a href="https://am.is.tuebingen.mpg.de/person/dkappler">Daniel Kappler</a>, 
               <a href="https://scholar.google.com/citations?user=jrfFYAIAAAAJ&hl=en">Alex Herzog</a>, 
               <a href="https://cs.stanford.edu/people/khansari/">Mohi Khansari</a>, 
               <a href="https://scholar.google.at/citations?user=SzHPa90AAAAJ&hl=de">Paul Wohlhart</a>, 
               <a href="https://www.yunfei-bai.com/">Yunfei Bai</a>, 
              <a href="http://www-clmc.usc.edu/Main/MrinalKalakrishnan">Mrinal Kalakrishnan</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1906.03352">arXiv</a>  / <a href="https://sites.google.com/view/watch-try-learn-project">project page</a><br>
              <div id="wtl" style="display:none">
              One-shot imitation learning enables robots to learn from a single demonstration, but doesn't allow them to improve through trial-and-error. Few-shot reinforcement learning allows for fast trial-and-error learning, but needs many trials to learn a new task with sparse rewards. We propose a simple and scalable approach to enable robots to meta-learn behavior from both demos and rewards, where a demonstration is used to indicate the task and trial-and-error is used to refine the skill.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('hvf').style.display = 'block';"
                onmouseout="document.getElementById('hvf').style.display='none';">
              <a href="https://arxiv.org/pdf/1909.05829.pdf">
                <papertitle>Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation</papertitle></a><br>
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1909.05829">arXiv</a>  / <a href="https://sites.google.com/stanford.edu/hvf">project page</a> / <a href="https://github.com/google-research/google-research/tree/master/hierarchical_foresight">code</a> 
            </div>
              <div id="hvf" style="display:none">
              We study how we can learn long-horizon vision-based tasks in self-supervised settings. Our approach, hierarchical visual foresight, can optimize for a sequence of subgoals that will make the task easier.
              </div><br>

            <div onmouseover="document.getElementById('atari').style.display = 'block';"
                onmouseout="document.getElementById('atari').style.display='none';">
              <a href="https://arxiv.org/pdf/1903.00374.pdf">
                <papertitle>Model-Based Reinforcement Learning for Atari</papertitle></a><br>
              <a href="https://scholar.google.com/citations?user=JWmiQR0AAAAJ&hl=en">Lukasz Kaiser</a>, 
              <a href="http://mb2.web.engr.illinois.edu/">Mohammad Babaeizadeh</a>, 
              <a href="https://www.mimuw.edu.pl/~pmilos/">Piotr Milos</a>, 
              <a href="https://scholar.google.com/citations?user=WuWWdKcAAAAJ&hl=en">Blazej Osinski</a>, 
              <a href="http://cs.illinois.edu/people/faculty/roy-campbell">Roy Campbell</a>,
              <a href="https://scholar.google.com/citations?user=p7hymWEAAAAJ&hl=en">Konrad Czechowski</a>, 
              <a href="http://www.dumitru.ca/">Dumitru Erhan</a>,
              <i>Chelsea Finn</i>,
              <a href="https://scholar.google.com/citations?user=QkYiDCwAAAAJ&hl=en">Piotr Kozakowski</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://github.com/afrozenator">Afroz Mohiuddin</a>, 
              <a href="https://scholar.google.com/citations?user=7vNmpO0AAAAJ&hl=en">Ryan Sepassi</a>, 
              <a href="https://sites.google.com/corp/view/gjt/">George Tucker</a>, 
              <a href="https://www.mimuw.edu.pl/~henrykm/resume.html">Henryk Michalewski</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <font color="#e37222"><strong>(Spotlight)</strong></font> <br>
              <a href="https://arxiv.org/abs/1903.00374">arXiv</a>  / <a href="https://goo.gl/itykP8">videos</a>
            </div>
              <div id="atari" style="display:none">
              </div><br>


            <div onmouseover="document.getElementById('videoflow').style.display = 'block';"
                onmouseout="document.getElementById('videoflow').style.display='none';">
              <a href="https://arxiv.org/pdf/1903.01434.pdf">
                <papertitle>VideoFlow: A Flow-Based Generative Model for Video</papertitle></a><br>
            <a href="https://scholar.google.com/citations?user=XQJN7dsAAAAJ&hl=en">Manoj Kumar</a>, 
              <a href="http://mb2.web.engr.illinois.edu/">Mohammad Babaeizadeh</a>, 
              <a href="http://www.dumitru.ca/">Dumitru Erhan</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="https://laurent-dinh.github.io/">Laurent Dinh</a>, 
              <a href="http://dpkingma.com/">Durk Kingma</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  <br>
              <a href="https://arxiv.org/abs/1903.01434">arXiv</a>  / <a href="https://sites.google.com/view/videoflow/home">videos</a> / <a href="https://github.com/google-research/google-research/tree/master/hierarchical_foresight">code</a> 
            </div>
              <div id="videoflow" style="display:none">
              </div><br>




            <div onmouseover="document.getElementById('lila').style.display = 'block';"
                onmouseout="document.getElementById('lila').style.display='none';">
              <a href="https://arxiv.org/pdf/1906.10187.pdf">
                <papertitle>Learning to Interactively Learn and Assist</papertitle></a><br>
              <a href="https://cs.stanford.edu/~woodward">Mark Woodward</a>, 
              <i>Chelsea Finn</i>,
              <a href="https://karolhausman.github.io/">Karol Hausman</a><br>
              <em>AAAI Conference on Artificial Intelligence</em>, 2020 <font color="#e37222"><strong>(Oral)</strong></font><br>
              <a href="https://arxiv.org/abs/1906.10187">arXiv</a>  / <a href="https://interactive-learning.github.io/">project page and interactive game</a> / <a href="https://www.youtube.com/watch?v=8yBvDBuAPrw&feature=youtu.be">video overview</a>
            </div>
            <div id="lila" style="display:none">  
              </div><br>



            <heading2><i>2019</i></heading2><br><br>

            <div onmouseover="document.getElementById('robonet').style.display = 'block';"
                onmouseout="document.getElementById('robonet').style.display='none';">
              <a href="https://arxiv.org/pdf/1910.11215.pdf">
                <papertitle>RoboNet: Large-Scale Multi-Robot Learning</papertitle></a><br>
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari</a>,
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              <a href="https://s-tian.github.io/">Stephen Tian</a>,
              <a href="http://surajnair.com">Suraj Nair</a>, 
              <a href="https://bucherb.github.io/">Bernadette Bucher</a>,
              <a href="https://sites.google.com/corp/view/karlschmeckpeper">Karl Schmeckpeper</a>,
              <a href="https://www.seas.upenn.edu/~sidsingh/">Siddharth Singh</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>Conference on Robot Learning (CoRL)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1910.11215">arXiv</a>  / <a href="https://www.robonet.wiki/">project page, code, data</a> / <a href="https://www.technologyreview.com/s/614668/welcome-to-robot-university-only-robots-need-apply/">press</a>
            </div>
            <div id="robonet" style="display:none">  
                The standard paradigm in robot learning is to set-up experiments in a single lab environment and train a robot from scratch from data collected in that setting. In contrast, essentially all machine learning fields accumulate and share large datasets across institutions, which enables training of models that generalize much more broadly. We aim to take a step in this direction by collecting a large dataset from 7 robot platforms across multiple institutions, which we call
                RoboNet. Critically, we find that pre-training on RoboNet enables us to generalize to entirely new robot platforms with less data than training from scratch.
              </div><br>


            <div onmouseover="document.getElementById('metaworld').style.display = 'block';"
                onmouseout="document.getElementById('metaworld').style.display='none';">
              <a href="https://arxiv.org/pdf/1910.10897.pdf">
                <papertitle>Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning</papertitle></a><br>
              <a href="https://cs.stanford.edu/~tianheyu/">Tianhe Yu</a>*,
              <a href="https://scholar.google.com/citations?user=eDQsOFMAAAAJ&hl=en/">Deirdre Quillen</a>*,
              <a href="https://zhanpenghe.github.io/">Zhanpeng He</a>*,
              <a href="https://ryanjulian.me/">Ryan Julian</a>,
              <a href="https://karolhausman.github.io/">Karol Hausman</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
              <em>Conference on Robot Learning (CoRL)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1910.10897">arXiv</a>  / <a href="https://meta-world.github.io/">project page</a> / <a href="https://github.com/rlworkgroup/metaworld">code</a>
            </div>
            <div id="metaworld" style="display:none">  
                While meta-RL is a promising approach for enabling robots to quickly learn new tasks based on previous experience, existing methods have tested on narrow distributions of tasks, hindering generalization. We develop a benchmark of 50 qualitatively distinct robotic manipulation tasks, with the goal of enabling future research on meta-RL that studies generalization to entirely new tasks.
              </div><br>


            <div onmouseover="document.getElementById('carml').style.display = 'block';"
                onmouseout="document.getElementById('carml').style.display='none';">
              <a href="https://arxiv.org/pdf/1912.04226">
                <papertitle>Unsupervised Curricula for Visual Meta-Reinforcement Learning</papertitle></a><br>
              <a href="https://ajabri.github.io/">Allan Jabri</a>, 
              <a href="https://kylehsu.me/">Kyle Hsu</a>, 
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, 
              <a href="https://ben-eysenbach.github.io/">Ben Eysenbach</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="#e37222"><strong>(Spotlight)</strong></font> <br>
              <a href="https://arxiv.org/abs/1912.04226">arXiv</a>  
            </div>
              <div id="carml" style="display:none">
              </div><br>


            <div onmouseover="document.getElementById('imaml').style.display = 'block';"
                onmouseout="document.getElementById('imaml').style.display='none';">
              <a href="https://arxiv.org/pdf/1909.04630.pdf">
                <papertitle>Meta-Learning with Implicit Gradients</papertitle></a><br>
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>*,
              <i>Chelsea Finn</i>*,
              <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1909.04630">arXiv</a>  / <a href="https://sites.google.com/view/imaml">project page</a>
            </div>
              <div id="imaml" style="display:none">
              Scaling meta-learning to long inner optimization procedures is difficult. We introduce iMAML, which meta-learns without differentiating through the inner optimization path using implicit differentiation. This allows you to use MAML with any inner loop optimizer. We also provide a theoretical analysis of the memory and computational requirements of a variety of meta-learning algorithms.
              </div><br>


            <div onmouseover="document.getElementById('hal').style.display = 'block';"
                onmouseout="document.getElementById('hal').style.display='none';">
              <a href="https://arxiv.org/pdf/1906.07343.pdf">
                <papertitle>Language as an Abstraction for Hierarchical Reinforcement Learning</papertitle></a><br>
              <a href="https://www.linkedin.com/in/yiding-jiang-600bb6116/">YiDing Jiang</a>,
              <a href="https://sites.google.com/corp/view/gugurus">Shixiang Gu</a>, 
              <a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1906.07343">arXiv</a>  / <a href="https://sites.google.com/view/hal-demo">project page</a> / <a href="https://github.com/google-research/clevr_robot_env">code</a>
            </div>
              <div id="hal" style="display:none">
              We propose to use language as an abstraction for hierarchical reinforcement learning as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. We also introduce a new open-source environment inspired by the CLEVR dataset for studying language and interaction.
              </div><br>


            <div onmouseover="document.getElementById('gmps').style.display = 'block';"
                onmouseout="document.getElementById('gmps').style.display='none';">
              <a href="https://arxiv.org/pdf/1904.00956.pdf">
                <papertitle>Guided Meta-Policy Search</papertitle></a><br>
              <a href="https://scholar.google.com/citations?user=Uly5spMAAAAJ&hl=en">Russell Mendonca</a>,
              <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>, 
              <a href="https://www.linkedin.com/in/rosen-kralev">Rosen Kralev</a>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="#e37222"><strong>(Spotlight)</strong></font> <br>
              <a href="https://arxiv.org/abs/1904.00956">arXiv</a>  / <a href="https://sites.google.com/berkeley.edu/guided-metapolicy-search">project page</a> / <a href="https://github.com/RussellM2020/GMPS">code</a>
            </div>
              <div id="gmps" style="display:none">
              We propose to learn a fast reinforcement learning procedure through imitation of expert policies that solve previously-seen tasks. Our approach is significantly more efficient and stable than prior methods, while scaling gracefully to vision-based and sparse reward tasks.
              </div><br>


            <div onmouseover="document.getElementById('pemirl').style.display = 'block';"
                onmouseout="document.getElementById('pemirl').style.display='none';">
              <a href="https://arxiv.org/pdf/1909.09314.pdf">
                <papertitle>Meta-Inverse Reinforcement Learning with Probabilistic Context Variables</papertitle></a><br>
              <a href="https://homes.cs.washington.edu/~aravraj/">Lantao Yu</a>*,
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>*,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Stefano Ermon</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019 <br>
              <a href="https://arxiv.org/abs/1909.09314">arXiv</a> 
            </div>
              <div id="pemirl" style="display:none">
              We show how we can learn a prior over reward functions from heterogeneous demonstration data using deep latent variable models. The proposed approach can learn reward functions for new tasks from a single demonstration, and use these rewards to learn a policy for the demonstrated task.
              </div><br>



            <div onmouseover="document.getElementById('hil').style.display = 'block';"
                onmouseout="document.getElementById('hil').style.display='none';">
              <a href="https://arxiv.org/pdf/1810.11043.pdf">
                <papertitle>One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks</papertitle></a><br>
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1810.11043">arXiv</a>  / <a href="https://sites.google.com/view/one-shot-hil">project page</a>
            </div>
              <div id="hil" style="display:none">
              We aim to learn multi-stage vision-based tasks on a real robot from a single video of a human performing the task. We propose a method that learns both how to learn primitive behaviors from video demonstrations and how to dynamically compose these behaviors to perform multi-stage tasks by "watching" a human demonstrator.
              </div><br>



            <div onmouseover="document.getElementById('viceq').style.display = 'block';"
                onmouseout="document.getElementById('viceq').style.display='none';">
              <a href="https://arxiv.org/pdf/1904.07854.pdf">
                <papertitle>End-to-End Robotic Reinforcement Learning without Reward Engineering</papertitle></a><br>
            <a href="https://people.eecs.berkeley.edu/~avisingh/">Avi Singh</a>,
              <a href="https://scholar.google.com/citations?user=n4kXFdsAAAAJ">Larry Yang</a>, 
              <a href="https://hartikainen.github.io/">Kristian Hartikainen</a>, 
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1904.07854">arXiv</a> / <a href="https://sites.google.com/view/reward-learning-rl">videos</a> / <a href="https://bair.berkeley.edu/blog/2019/05/28/end-to-end/">blog post</a> / <a href="https://github.com/avisingh599/reward-learning-rl">code</a>
            </div>
              <div id="viceq" style="display:none">
                  Motivated by the challenge of learning behavior efficiently and the manual effort that often goes into designing reward functions, we extend prior work on variational inverse control with events (VICE) to the off-policy RL setting and show how robots use active queries to learn the reward function more efficiently and robustly.
              </div><br>




            <div onmouseover="document.getElementById('gvf').style.display = 'block';"
                onmouseout="document.getElementById('gvf').style.display='none';">
              <a href="https://arxiv.org/pdf/1904.05538.pdf">
                <papertitle>Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight</papertitle></a><br>
              <a href="https://anxie.github.io/">Annie Xie</a>,
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1904.05538">arXiv</a> / <a href="https://sites.google.com/corp/view/gvf-tool">videos</a>
            </div>
              <div id="gvf" style="display:none">
                We study how robots can learn to use tools. With a combination of autonomous robot interaction (to learn about cause and effect) and teleoperated demonstrations (to learn about how to use tools), we show that robots can figure out how to solve tasks using novel tools and even improvise when conventional tools aren't available.
              </div><br>




            <div onmouseover="document.getElementById('dpn').style.display = 'block';"
                onmouseout="document.getElementById('dpn').style.display='none';">
              <a href="https://arxiv.org/pdf/1902.05542.pdf">
                <papertitle>Unsupervised Visuomotor Control through Distributional Planning Networks</papertitle></a><br>
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>,
              <a href="https://profiles.stanford.edu/gleb-shevchuk">Gleb Shevchuk</a>,
              <a href="http://dorsa.fyi">Dorsa Sadigh</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1902.05542">arXiv</a>  / <a href="https://sites.google.com/view/dpn-public">project page</a>
            </div>
              <div id="dpn" style="display:none">
                How can robots learn without rewards? We train for a metric such that optimizing that metric leads to actions that reach the goal, which we can train for with random interaction data. By doing so, robots can learn a variety of image-based tasks without any human supervision.
              </div><br>





            <div onmouseover="document.getElementById('ftml').style.display = 'block';"
                onmouseout="document.getElementById('ftml').style.display='none';">
              <a href="https://arxiv.org/pdf/1902.08438 .pdf">
                <papertitle>Online Meta-Learning</papertitle></a><br>
              <i>Chelsea Finn</i>*,
              <a href="https://homes.cs.washington.edu/~aravraj/">Aravind Rajeswaran</a>*,
              <a href="https://homes.cs.washington.edu/~sham/">Sham Kakade</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1902.08438 ">arXiv</a>  
            </div>
              <div id="ftml" style="display:none">
                We aim to bring together meta-learning (i.e., few-shot learning) and online learning, both in theory and in practice.
              </div><br>


            <div onmouseover="document.getElementById('pearl').style.display = 'block';"
                onmouseout="document.getElementById('pearl').style.display='none';">
              <a href="https://arxiv.org/pdf/1903.08254.pdf">
                <papertitle>Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables</papertitle></a><br>
              <a href="http://people.eecs.berkeley.edu/~rakelly">Kate Rakelly</a>*,
              <a href="https://scholar.google.com/citations?user=1O83J5MAAAAJ&hl=en/">Aurick Zhou</a>*,
              <a href="https://scholar.google.com/citations?user=eDQsOFMAAAAJ&hl=en/">Deirdre Quillen</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1903.08254">arXiv</a> / <a href="https://github.com/katerakelly/oyster">code</a>
            </div>
              <div id="pearl" style="display:none">
                We introduce PEARL, a method that leverages off-policy learning and a probabilistic belief over the task to make meta-reinforcement learning 20-100X more sample efficient.
              </div><br>


            <div onmouseover="document.getElementById('mirl').style.display = 'block';"
                onmouseout="document.getElementById('mirl').style.display='none';">
              <a href="https://arxiv.org/pdf/1805.12573.pdf">
                <papertitle>Learning a Prior over Intent via Meta-Inverse Reinforcement Learning</papertitle></a><br>
              <a href="http://kelvinxu.github.io/">Kelvin Xu</a>, 
              Ellis Ratner, 
              <a href="http://people.eecs.berkleey.edu/~anca/">Anca Dragan</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Machine Learning (ICML)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1805.12573">arXiv</a> <br>
              <div id="mirl" style="display:none">
              Learning the objective underlying example behavior is a challenging, under-defined problem, particularly from only a few demonstrations. However, there is structure among the type of behaviors that we might want agents to learn. We learn this structure from demonstrations across many tasks, acquiring a prior over intentions, and use this learned prior to infer reward functions for new tasks from only a few demonstrations.
              </div>
            </div><br>



            <div onmouseover="document.getElementById('feel').style.display = 'block';"
                onmouseout="document.getElementById('feel').style.display='none';">
              <a href="https://arxiv.org/pdf/1903.04128.pdf">
                <papertitle>Manipulation by Feel: Touch-Based Control with Deep Predictive Models</papertitle></a><br>
              Stephen Tian*,
              <a href="https://febert.github.io/">Frederik Ebert*</a>,
              <a href="https://people.eecs.berkeley.edu/~dineshjayaraman/">Dinesh Jayaraman</a>, 
              <a href="https://redwood.berkeley.edu/people/mayur-mudigonda/">Mayur Mudigonda</a>,
              <i>Chelsea Finn</i>,
              <a href="https://www.robertocalandra.com/about/">Roberto Calandra</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1903.04128">arXiv</a> / <a href="https://sites.google.com/view/deeptactilempc">videos</a> / <a href="https://bair.berkeley.edu/blog/2019/03/21/tactile/">blog post</a>
              <br>
              <div id="feel" style="display:none">
                  We show how we can use the visual foresight method to enable robots to manipulate objects entirely by feel, without vision.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('norml').style.display = 'block';"
                onmouseout="document.getElementById('norml').style.display='none';">
              <a href="https://arxiv.org/pdf/1903.01063.pdf">
                <papertitle>NoRML: No-Reward Meta Learning</papertitle></a><br>
              <a href="https://yxyang.github.io/">Yuxiang Yang</a>,
              <a href="https://ai.google/research/people/KenCaluwaerts/">Ken Caluwaerts</a>, 
              <a href="https://ai.google/research/people/105360/">Atil Iscen</a>,
              <a href="http://www.jie-tan.net/">Jie Tan</a>,
              <i>Chelsea Finn</i>
              <br>
              <em>International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1903.01063">arXiv</a> / <a href="https://sites.google.com/view/noreward-meta-rl/">project page</a>
              <br>
              <div id="norml" style="display:none">
                  We identify key shortcomings of existing meta-reinforcement learning algorithms in the setting of adapting to new dynamics, and develop a new method that can effectively adapt to new dynamics in a model-free way without reward information, by meta-learning an advantage function.
              </div>
            </div><br>



            <div onmouseover="document.getElementById('ladr').style.display = 'block';"
                onmouseout="document.getElementById('ladr').style.display='none';">
              <a href="https://arxiv.org/pdf/1803.11347.pdf">
                <papertitle>Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~nagaban2/">Anusha Nagabandi*</a>, 
              <a href="https://iclavera.github.io/">Ignasi Clavera*</a>,
              Simin Liu,
              <a href="https://people.eecs.berkeley.edu/~ronf/">Ron Fearing</a>, 
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1803.11347">arXiv</a> / <a href="https://sites.google.com/view/learning2adapt">videos</a> / <a href="https://github.com/iclavera/learning_to_adapt">code</a>
              <br>
              <div id="ladr" style="display:none">
              We propose a method that learns how to adapt <i>online</i> to new situations and perturbations, through meta reinforcement learning. Unlike prior meta-RL methods,
              our approach is model-based, making it sample-efficient during meta-training and thus practical for real world problems.
              </div>
            </div><br>


            <div onmouseover="document.getElementById('uml').style.display = 'block';"
                onmouseout="document.getElementById('uml').style.display='none';">
              <a href="https://arxiv.org/pdf/1810.02334.pdf">
                <papertitle>Unsupervised Learning via Meta-Learning </papertitle></a><br>
              <a href="https://kylehsu.me/">Kyle Hsu</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1810.02334">arXiv</a> / <a href="https://sites.google.com/view/unsupervised-via-meta">project page</a> / <a href="https://github.com/hsukyle/cactus-maml">code</a>
              <br> 
              <div id="uml" style="display:none">
              We propose CACTUs, an unsupervised learning algorithm that learns to learn tasks constructed from unlabeled data. CACTUs leads to significantly more effective downstream learning and enables few-shot learning without requiring labeled meta-learning datasets.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('o2p2').style.display = 'block';"
                onmouseout="document.getElementById('o2p2').style.display='none';">
              <a href="https://arxiv.org/pdf/1812.10972.pdf">
                <papertitle>Reasoning About Physical Interactions with Object-Oriented Prediction and Planning</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~janner/">Michael Janner</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="http://billf.mit.edu/">Bill Freeman</a>,
              <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>,
              <i>Chelsea Finn</i>,
              <a href="https://jiajunwu.com/">Jiajun Wu</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1812.10972">arXiv</a> / <a href="https://people.eecs.berkeley.edu/~janner/o2p2/">project page</a>
              <br> 
              <div id="o2p2" style="display:none">
              We develop an object-centric model of visual interactions, illustrate the model's internal representations of objects and physics, and use it to accomplish a variety of long-horizon block-stacking tasks on a robot.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('mole').style.display = 'block';"
                onmouseout="document.getElementById('mole').style.display='none';">
              <a href="https://arxiv.org/pdf/1812.07671.pdf">
                <papertitle>Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~nagaban2/">Anusha Nagabandi</a>, 
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2019  <br>
              <a href="https://arxiv.org/abs/1812.07671">arXiv</a> / <a href="https://sites.google.com/berkeley.edu/onlineviameta">project page</a>
              <br> 
              <div id="mole" style="display:none">
              We show that pre-training model parameters with meta-learning (using MAML) can enable effective online learning with neural networks, which we apply to model-based RL problems with non-stationary dynamics.
              </div>
            </div><br>


            <heading2><i>2018</i></heading2><br><br>

            <div onmouseover="document.getElementById('journalvf').style.display = 'block';"
                onmouseout="document.getElementById('journalvf').style.display='none';">
              <a href="https://arxiv.org/pdf/1812.00568.pdf">
                <papertitle>Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control
</papertitle></a><br>
              <a href="https://febert.github.io/">Frederik Ebert</a>*, 
              <i>Chelsea Finn</i>*,
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari</a>,
              <a href="https://anxie.github.io/">Annie Xie</a>,
              <a href="https://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>
              <br>
              <a href="https://arxiv.org/abs/1812.00568">arXiv</a>  / <a href="https://sites.google.com/corp/view/visualforesight">project page</a> / <a href="https://github.com/SudeepDasari/visual_foresight">code</a> / <a href="https://sites.google.com/corp/berkeley.edu/robotic-interaction-datasets/home">data</a>
            </div>
              <div id="journalvf" style="display:none">
              We provide a unified overview of our work on visual foresight, and show new experiments that show how a single video prediction model can be used to solve many different vision-based tasks, including deformable object manipulation tasks involving towels, shorts, and shirts.
              </div><br>


            <div onmouseover="document.getElementById('savp').style.display = 'block';"
                onmouseout="document.getElementById('savp').style.display='none';">
              <a href="https://arxiv.org/pdf/1804.01523.pdf">
                <papertitle>Stochastic Adversarial Video Prediction</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>, 
              <a href="https://richzhang.github.io/">Richard Zhang</a>, 
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
              <a href="https://arxiv.org/abs/1804.01523">arXiv</a> / <a href="https://alexlee-gk.github.io/video_prediction/">videos</a> / <a href="https://github.com/alexlee-gk/video_prediction">code </a> <br>
              <div id="savp" style="display:none">
              We combine latent variable models with adversarial training to build a video prediction model that produces predictions that look more realistic to human raters and better cover the range of possible futures.
              </div>
            </div><br>


            <div onmouseover="document.getElementById('platipus').style.display = 'block';"
                onmouseout="document.getElementById('platipus').style.display='none';">
              <a href="https://arxiv.org/pdf/1806.02817.pdf">
                <papertitle>Probabilistic Model-Agnostic Meta-Learning</papertitle></a><br>
              <i>Chelsea Finn</i>*,
              <a href="http://kelvinxu.github.io">Kelvin Xu</a>*, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a><br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1806.02817">arXiv</a> / <a href="https://sites.google.com/view/probabilistic-maml/">supplementary website</a>
              <br> 
              <div id="platipus" style="display:none">
              Few-shot learning problems can be ambiguous. We propose a modification of the MAML algorithm that can handle ambiguity by sampling different multiple classifiers. Our approach uses a Bayesian formulation of meta-learning, building upon prior work on hierarchical Bayesian models and variational inference.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('thesis').style.display = 'block';"
                onmouseout="document.getElementById('thesis').style.display='none';">
              <a href="_files/dissertation.pdf">
                <papertitle>Learning to Learn with Gradients</papertitle></a><br>
              <i>Chelsea Finn</i><br>
              <em>PhD Dissertation</em>, 2018 <br>
              <div id="thesis" style="display:none">
              We develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Finally, we show how these methods can be extended for applications in motor control by combining  elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('flo').style.display = 'block';"
                onmouseout="document.getElementById('flo').style.display='none';">
              <a href="https://arxiv.org/pdf/1810.00482">
                <papertitle>Few-Shot Goal Inference for Visuomotor Learning and Planning</papertitle></a><br>
            <a href="https://scholar.google.com/citations?user=lns9LUsAAAAJ&hl=en">Annie Xie</a>,
              <a href="https://people.eecs.berkeley.edu/~avisingh/">Avi Singh</a>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>Conference on Learning (CoRL)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1810.00482">arXiv</a> / <a href="https://sites.google.com/view/few-shot-goals">videos</a> / <a href="https://github.com/anxie/meta_classifier">code</a>
              <br> 
              <div id="flo" style="display:none">
              Specifying a reward or objective in the real world is hard. We propose a method that enables a robot to learn an objective from a few images of success by leveraging a dataset of positive and negative examples of previous tasks. We show how the objectives learned with our method can be used for both planning in the real world and reinforcement learning in simulation.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('retry').style.display = 'block';"
                onmouseout="document.getElementById('retry').style.display='none';">
              <a href="https://arxiv.org/pdf/1810.03043">
                <papertitle>Robustness via Retrying: Closed-Loop Robotic Manipulation via Self-Supervised Learning</papertitle></a><br>
              <a href="https://febert.github.io/">Frederik Ebert</a>, 
              Sudeep Dasari,
              <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>Conference on Learning (CoRL)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1810.03043">arXiv</a> / <a href="https://sites.google.com/view/robustness-via-retrying">video</a> 
              <br>
              <div id="retry" style="display:none">
              Planning with video prediction models trained on self-supervised data allows robots to learn diverse manipulation skills. However, to recover from disturbances and inaccurate predictions, we need to track pixels continuously to evaluate the planning objective at each timestep. We propose a self-supervised image-to-image registration model that enables robust behavior.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('upn').style.display = 'block';"
                onmouseout="document.getElementById('upn').style.display='none';">
              <a href="https://arxiv.org/pdf/1804.00645.pdf">
                <papertitle>Universal Planning Networks</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~aravind/">Aranvind Srinivas</a>,
              <a href="https://ajabri.github.io/">Allan Jabri</a>, 
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <i>Chelsea Finn</i><br>
              <em>International Conference on Machine Learning (ICML)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1804.00645">arXiv</a> / <a href="https://sites.google.com/view/upn-public/home">videos</a> / <a href="https://github.com/aravind0706/upn">code</a>
              <br>
              <div id="upn" style="display:none">
              We propose to embed differentiable planning within a goal-directed policy, integrating planning and representation learning. Our approach optimizes for
              representations that lead to effective goal-based planning for visual tasks. Our results show that the representation not only allow for effective goal-based 
              planning through imitation, but also transfers to more complex robot morphologies and action spaces.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('daml').style.display = 'block';"
                onmouseout="document.getElementById('daml').style.display='none';">
              <a href="https://arxiv.org/pdf/1802.01557">
                <papertitle>One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning</papertitle></a><br>
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>*,
              <i>Chelsea Finn</i>*,
              <a href="https://scholar.google.com/citations?user=lns9LUsAAAAJ&hl=en">Annie Xie</a>,
              Sudeep Dasari,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1802.01557">arXiv</a> / <a href="https://sites.google.com/view/daml">video</a> / <a href="https://github.com/tianheyu927/mil">code</a> / <a href="http://bair.berkeley.edu/blog/2018/06/28/daml/">blog post</a>
              <br> 
              <div id="daml" style="display:none">
              We develop a domain-adaptive meta-learning method that allows for one-shot learning under domain shift. We show that our method can enable a robot to learn to maneuver a new object after seeing just
              one video of a human performing the task with that object.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('mlu').style.display = 'block';"
                onmouseout="document.getElementById('mlu').style.display='none';">
              <a href="https://arxiv.org/pdf/1710.11622.pdf">
                <papertitle>Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm</papertitle></a><br>
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2018  <br>
                <a href="https://arxiv.org/abs/1710.11622">arXiv</a>
              <br> 
              <div id="mlu" style="display:none">
              We show that model-agnostic meta-learning (MAML), which embeds gradient descent into the meta-learning algorithm, can be as expressive as black-box meta-learners: both can approximate any learning algorithm.
              Furthermore, we empirically show that MAML consistently finds learning strategies that generalize to new tasks better than recurrent meta-learners.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('llama').style.display = 'block';"
                onmouseout="document.getElementById('llama').style.display='none';">
              <a href="https://arxiv.org/pdf/1801.08930.pdf">
                <papertitle>Recasting Gradient-Based Meta-Learning as Hierarchical Bayes</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~eringrant/">Erin Grant</a>, 
              <i>Chelsea Finn</i>, 
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> ,
              <a href="https://www.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="http://cocosci.berkeley.edu/tom/index.php">Tom Griffiths</a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2018  <br>
                <a href="https://arxiv.org/abs/1801.08930">arXiv</a>
              <br> 
              <div id="llama" style="display:none">
             We reformulate the model-agnostic meta-learning algorithm (MAML) as a method for probabilistic inference in a hierarchical Bayesian model.
             Unlike prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to large function approximators, like neural networks.
             Our interpretation sheds light on the meta-learning procedure and allows us to derive an improved version of the MAML algorithm.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('sv2p').style.display = 'block';"
                onmouseout="document.getElementById('sv2p').style.display='none';">
              <a href="https://arxiv.org/pdf/1710.11252.pdf">
                <papertitle>Stochastic Variational Video Prediction</papertitle></a><br>
              <a href="http://mb2.web.engr.illinois.edu/">Mohammad Babaeizadeh</a>, 
              <i>Chelsea Finn</i>, 
              <a href="http://www.dumitru.ca/">Dumitru Erhan</a>,
              <a href="http://cs.illinois.edu/people/faculty/roy-campbell">Roy Campbell</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2018  <br>
                <a href="https://arxiv.org/abs/1710.11252">arXiv</a>
                /
                <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/video/sv2p.py">code</a>
                /
                <a href="https://goo.gl/iywUHc">video results</a>
              <br> 
              <div id="sv2p" style="display:none">
              We present a stochastic video prediction method, SV2P, that builds upon the conditional variational autoencoder to make stochastic predictions of future video.
              We find that pretraining is crucial for enabling stochasticity. Our experiments demonstrate stochastic multi-frame predictions on three real world video datasets.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('deirdre').style.display = 'block';"
                onmouseout="document.getElementById('deirdre').style.display='none';">
              <a href="https://arxiv.org/pdf/1802.10264.pdf">
                <papertitle>Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods</papertitle></a><br>
              <a href="https://scholar.google.com/citations?user=eDQsOFMAAAAJ&hl=en/">Deirdre Quillen</a>*,
              <a href="http://evjang.com/">Eric Jang*</a>,
              <a href="https://research.google.com/pubs/105364.html">Ofir Nachum*</a>, 
              <i>Chelsea Finn</i>, 
              <a href="https://research.google.com/pubs/JulianIbarz.html">Julian Ibarz</a> ,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2018  <br>
              <a href="https://arxiv.org/abs/1802.10264">arXiv</a> / <a href="https://sites.google.com/view/grasping-icra2018">project page</a> / <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/kuka_diverse_object_gym_env.py">benchmark code</a>
              <br> 
              <div id="deirdre" style="display:none">
              We propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. 
              Our results indicate that several simple methods provide a surprisingly strong 
              competitor to popular deep RL algorithms such as double Q-learning, and our analysis sheds light on the relative tradeoffs between the methods.
              </div>
            </div><br>

            <heading2><i>2017</i></heading2><br><br>

            <div onmouseover="document.getElementById('mil').style.display = 'block';"
                onmouseout="document.getElementById('mil').style.display='none';">
              <a href="https://arxiv.org/pdf/1709.04905.pdf">
                <papertitle>One-Shot Visual Imitation Learning via Meta-Learning</papertitle></a><br>
              <i>Chelsea Finn</i>*,
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>*,
              <a href="http://tianhaozhang.com/">Tianhao Zhang</a>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2017 <font color="#e37222"><strong>(Long Talk)</strong></font> <br>
              <strong style="color:#e37222">Oral presentation at the <a href="https://sites.google.com/view/deeprl-symposium-nips2017/home">NIPS 2017 Deep Reinforcement Learning Symposium</a></strong><br>
                <a href="https://arxiv.org/abs/1709.04905">arXiv</a>
                /
                <a href="https://github.com/tianheyu927/mil">code</a>
                /
                <a href="https://sites.google.com/view/one-shot-imitation">result video</a>
                /
                <a href="https://youtu.be/_9Ny2ghjwuY?t=1h57m38s">talk video</a>
              <div id="mil" style="display:none">
              Using demonstration data from a variety of tasks, our method enables a real robot to learn a new related skill, trained end-to-end, using a single visual demonstration of the skill. Our approach also allows for the provided demonstration to be a raw video, without access to the joint trajectory or controls applied to the robot arm.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('tsc').style.display = 'block';"
                onmouseout="document.getElementById('tsc').style.display='none';">
              <a href="https://arxiv.org/pdf/1710.05268.pdf">
                <papertitle>Self-Supervised Visual Planning with Temporal Skip Connections</papertitle></a><br>
              <a href="https://febert.github.io/">Frederik Ebert</a>, <i>Chelsea Finn</i>, <a href="http://people.eecs.berkeley.edu/~alexlee_gk/">Alex Lee</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2017 <font color="#e37222"><strong>(Long Talk)</strong></font> <br>
                <a href="https://arxiv.org/abs/1710.05268">arXiv</a>
                /
                <a href="https://github.com/febert/visual_mpc">code</a>
                /
                <a href="https://sites.google.com/view/sna-visual-mpc">video results and data</a>
              <br> 
              <div id="tsc" style="display:none">
              We present three simple improvements to our prior work on self-supervised visual foresight that lead to substantially better visual planning capabilities. Our
              method can perform tasks that require longer-term planning and involve multiple objects.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('maml').style.display = 'block';"
                onmouseout="document.getElementById('maml').style.display='none';">

              <a href="https://arxiv.org/pdf/1703.03400.pdf">
                <papertitle>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</papertitle></a><br>
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Machine Learning (ICML)</em>, 2017 <br>
                <a href="https://arxiv.org/abs/1703.03400">arXiv</a>
                /
                <a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/">blog post</a>
                /
                <a href="https://github.com/cbfinn/maml">code</a>
                /
                <a href="https://sites.google.com/view/maml">video results</a>
              <br> 
              <div id="maml" style="display:none">
              We propose a model-agnostic algorithm for meta-learning, where a model's parameters
              are trained such that a small number of gradient updates with a small amount of training data from a new task
              will produce good generalization performance on that task. Our method learns a classifier that can recognize
              images of new characters using only a few examples, and a policy that can rapidly adapt
              its behavior in simulated locomotion tasks.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('ssrl').style.display = 'block';"
                onmouseout="document.getElementById('ssrl').style.display='none';">
              <a href="https://arxiv.org/pdf/1612.00429.pdf">
                <papertitle>Generalizing Skills with Semi-Supervised Reinforcement Learning</papertitle></a><br>
              <i>Chelsea Finn</i>, 
              <a href="http://tianheyu927.github.io">Tianhe Yu</a>, 
              <a href="http://people.eecs.berkeley.edu/~justinjfu/">Justin Fu</a>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="https://arxiv.org/abs/1612.00429">arXiv</a>
                /
                <a href="https://sites.google.com/site/semisupervisedrl">video results</a>
                /
                <a href="https://github.com/cbfinn/gps/tree/ssrl">code</a>
              <br>
              <div id="ssrl" style="display:none">
              We formalize the problem of semi-supervised reinforcement learning (SSRL), motivated by real-world scenarios where reward information
              is only available in a limited set of scenarios such as when a human supervisor is present, or in a controlled laboratory setting.
              We develop a simple algorithm for SSRL based on inverse reinforcement learning and show that it can improve performance by using
              'unlabeled' experience.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('foresight').style.display = 'block';"
                onmouseout="document.getElementById('foresight').style.display='none';">
              <a href="https://arxiv.org/pdf/1610.00696.pdf">
                <papertitle>Deep Visual Foresight for Planning Robot Motion</papertitle></a><br>
              <i>Chelsea Finn</i>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2017 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
              <strong style="color:#e37222">Best Cognitive Robotics Paper Finalist</strong><br>
                <a href="http://arxiv.org/abs/1610.00696">arXiv</a>
                /
                <a href="https://sites.google.com/site/robotforesight/">video</a>
              <br>
              <div id="foresight" style="display:none">
              We combine an action-conditioned predictive model of images, "visual foresight," with model-predictive control for planning how
              to push objects. The method is entirely self-supervised, requiring minimal human involvement.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('rfgps').style.display = 'block';"
                onmouseout="document.getElementById('rfgps').style.display='none';">
                <a href="https://arxiv.org/pdf/1610.01112.pdf">
                <papertitle>Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States</papertitle></a><br>
              <a href="http://www.cs.washington.edu/node/9179">William Montgomery*</a>,
              <a href="https://www.linkedin.com/in/anurag-ajay-3060b080">Anurag Ajay*</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2017 <br> 
                <a href="http://arxiv.org/abs/1610.01112">arXiv</a>
                /
                <a href="http://sites.google.com/site/resetfreegps">video</a>
                /
                <a href="https://github.com/cbfinn/gps/tree/rfgps">code</a>
              <br>
              <div id="rfgps" style="display:none">
              We present a new guided policy search algorithm that allows the method to be used in domains where the initial conditions are stochastic, which makes the method
              more applicable to general reinforcement learning problems and improves generalization performance in our robotic manipulation experiments.
              </div>
            </div><br>

            <heading2><i>2016</i></heading2><br><br>

            <div onmouseover="document.getElementById('gans').style.display = 'block';"
                onmouseout="document.getElementById('gans').style.display='none';">
          <a href="https://arxiv.org/pdf/1611.03852.pdf" id="ganirl">
              <papertitle>A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models</papertitle></a><br>
            <i>Chelsea Finn</i>*, <a href="https://paulfchristiano.com/">Paul Christiano*</a>,
            <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
            <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br>
          <em>NIPS Workshop on Adversarial Training</em>, 2016<br>
          <a href="https://arxiv.org/abs/1611.03852">arXiv</a>
          <br>
          <div id="gans" style="display:none">
          We show that a sample-based algorithm for maximum entropy inverse reinforcement learning (MaxEnt IRL) corresponds to a generative adversarial network (GAN) with a particular choice of discriminator.
          Since MaxEnt IRL is simply an energy-based model (EBM) for behavior, we further show that GANs optimize EBMs with the corresponding discriminator,
          pointing to a simple and scalable EBM training procedure using GANs.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('aosl').style.display = 'block';"
                onmouseout="document.getElementById('aosl').style.display='none';">
          <a href="https://arxiv.org/pdf/1702.06559.pdf" id="activelearn">
              <papertitle>Active One-Shot Learning</papertitle></a><br>
            <a href="https://cs.stanford.edu/~woodward">Mark Woodward</a>, <i>Chelsea Finn</i> <br>
          <em>NIPS Deep Reinforcement Learning Workshop</em>, 2016 <font color="#e37222"><strong>(Oral)</strong></font>  <br>
          <a href="https://arxiv.org/abs/1702.06559">arXiv</a> / <a href="https://www.youtube.com/watch?v=CzQSQ_0Z-QU">video description</a> / <a href="https://cs.stanford.edu/~woodward/papers/active_one_shot_learning_2016_poster.pdf">poster</a>
          <br>
              <div id="aosl" style="display:none">
          We propose a technique for learning an active learning strategy by combining one-shot learning and reinforcement learning, and allowing the model
          to decide, during classification, which examples are worth labeling. Our experiments demonstrate that our model can trade-off
          accuracy and label requests based on the reward function provided.
              </div>
            </div><br>


            <div onmouseover="document.getElementById('cdna').style.display = 'block';"
                onmouseout="document.getElementById('cdna').style.display='none';">
              <a href="https://arxiv.org/pdf/1605.07157.pdf">
                <papertitle>Unsupervised Learning for Physical Interaction through Video Prediction</papertitle></a><br>
              <i>Chelsea Finn</i>, <a href="http://goodfeli.github.io/">Ian Goodfellow</a>, <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a> <br>
              <em>Neural Information Processing Systems (NIPS)</em>, 2016 <br> <!-- &nbsp; <font color="red"><strong>(Oral Presentation)</strong></font> <br-->
                <a href="http://arxiv.org/abs/1605.07157">arXiv</a>
                /
                <a href="https://sites.google.com/site/robotprediction/">videos</a>
                /
                <a href="https://sites.google.com/site/brainrobotdata/home/">data</a>
                /
                <a href="https://github.com/tensorflow/models/tree/master/research/video_prediction">code</a>
              <br>
              <div id="cdna" style="display:none">
              Our video prediction method predicts a transformation to apply to the previous image, rather than pixels values directly, leading to significantly improved multi-frame video prediction. We also introduce
              a dataset of 50,000 robotic pushing sequences, consisting of over 1 million frames.
              </div>
            </div><br>
<div onmouseover="document.getElementById('sim2real').style.display = 'block';"
    onmouseout="document.getElementById('sim2real').style.display='none';">

              <a href="https://arxiv.org/pdf/1511.07111.pdf">
                <papertitle>Adapting Deep Visuomotor Representations with Weak Pairwise Constraints</papertitle></a><br>
              <a href="https://github.com/erictzeng">Eric Tzeng</a>,
              <a href="http://colinedevin.com/">Coline Devin</a>,
              <a href="http://cs.stanford.edu/~jhoffman/">Judy Hoffman</a>,
              <i>Chelsea Finn</i>,
              <a href="http://people.eecs.berkeley.edu/~pabbeel">Pieter Abbeel</a>,
              <a href="http://people.eecs.berkeley.edu/~svlevine">Sergey Levine</a>,
              <a href="http://vision.cs.uml.edu/ksaenko.html">Kate Saenko</a>,
              <a href="https://www.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><br>
              <!--a href="">Sergey Levine</a-->
                <em>Workshop on the Algorithmic Foundations of Robotics (WAFR)</em>, 2016<br>
                <a href="http://arxiv.org/abs/1511.07111">arXiv</a>
              <br>
              <div id="sim2real" style="display:none">
              Collecting real-world robotic experience for learning an initial visual representation can be expensive. Instead, we show that it is possible to learn
              a suitably good initial representation using data collected largely in simulation.</p>
              </div>
            </div><br>

            <div onmouseover="document.getElementById('gcl').style.display = 'block';"
                onmouseout="document.getElementById('gcl').style.display='none';">

     <a href="http://jmlr.org/proceedings/papers/v48/finn16.pdf">
       <papertitle>Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization</papertitle></a><br>
     <i>Chelsea Finn</i>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
     <em>International Conference on Machine Learning (ICML)</em>, 2016 <br>
     <strong style="color:#e37222">Oral presentation at the <a href="https://sites.google.com/site/nips2016deeplearnings/home">NIPS 2016 Deep Learning Symposium</a></strong><br>
     <a href="http://arxiv.org/abs/1603.00448">arXiv</a> /
     <a href="http://rll.berkeley.edu/gcl">video results</a> / 
     <a href="https://github.com/justinjfu/inverse_rl">code</a> /
     <a href="http://techtalks.tv/talks/guided-cost-learning-deep-inverse-optimal-control-via-policy-optimization/62472/">talk video</a>
     <br>
              <div id="gcl" style="display:none">
     We propose an method for Inverse Reinforcement Learning (IRL) that can handle unknown dynamics and scale to flexible, nonlinear cost functions. We evaluate our algorithm on a series of simulated tasks and real-world robotic manipulation problems, including pouring and inserting dishes into a rack.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('e2e').style.display = 'block';"
                onmouseout="document.getElementById('e2e').style.display='none';">

              <a href="http://www.jmlr.org/papers/volume17/15-522/15-522.pdf">
                <papertitle>End-to-End Training of Deep Visuomotor Policies</papertitle></a><br>
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine*</a>,
              <i>Chelsea Finn</i>*, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
              <strong style="color:#e37222">CCC Blue Sky Ideas <a href="http://www.cccblog.org/2015/08/03/blue-sky-ideas-aaai-rss-special-workshop-on-the-50th-anniversary-of-shakey/">Award</a></strong><br>
              <em>Journal of Machine Learning Research (JMLR)</em>, 2016 <br>
                <a href="https://arxiv.org/abs/1504.00702">arXiv</a> /
                <a href="https://sites.google.com/site/visuomotorpolicy/">video</a> /
                <a href="http://rll.berkeley.edu/deeplearningrobotics">project page</a> /
<a href="http://rll.berkeley.edu/gps">code</a>
              <br>
              <div id="e2e" style="display:none">
              We demonstrate a deep neural network trained end-to-end, from perception to controls, for robotic manipulation tasks.
              </div>
            </div><br>

            <div onmouseover="document.getElementById('dsae').style.display = 'block';"
                onmouseout="document.getElementById('dsae').style.display='none';">
          
            <a href="http://arxiv.org/pdf/1509.06113.pdf">
              <papertitle>Deep Spatial Autoencoders for Visuomotor Learning</papertitle>
            </a><br>
            <i>Chelsea Finn</i>, <a href="https://sites.google.com/site/xinyutan17/">Xin Yu Tan</a>,
            <a href="http://www.rockyduan.com/">Yan Duan</a>, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
            <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
            <em>International Conference on Robotics and Automation (ICRA)</em>, 2016 <br>
            <a href="http://arxiv.org/abs/1509.06113">arXiv</a> /
            <a href="http://rll.berkeley.edu/dsae/">video</a>
          <br>
              <div id="dsae" style="display:none">
          We learn a lower dimensional visual state-space without supervision using deep spatial autoencoders, and use it to learn nonprehensile manipulation
          tasks, such as pushing a lego block and scooping a bag into a bowl.
              </div>
            </div><br>



          
            <div onmouseover="document.getElementById('cms').style.display = 'block';"
                onmouseout="document.getElementById('cms').style.display='none';">
            <a href="https://arxiv.org/pdf/1507.01273.pdf" id="MCG_journal">
              <papertitle>Learning Deep Neural Network Policies with Continuous Memory States</papertitle>
            </a>
            <br>
            <a href="http://marvinzhang.com/">Marvin Zhang</a>, <a href="https://people.eecs.berkeley.edu/~zmccarthy/">Zoe McCarthy</a>,
            <i>Chelsea Finn</i>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
            <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a><br>
  <em>International Conference on Robotics and Automation (ICRA)</em>, 2016<br>
  <a href="https://arxiv.org/abs/1507.01273">arXiv</a> /
  <a href="http://rll.berkeley.edu/gpsrnn/">video</a>
          <br>
              <div id="cms" style="display:none">
          We propose a method for learning recurrent neural network policies using continuous memory states. The method learns to store information in and use the memory states
          using trajectory optimization. Our method outperforms vanilla RNN and LSTM baselines.
              </div>
            </div><br>

            <heading2><i>2015</i></heading2><br><br>

            <div onmouseover="document.getElementById('junct').style.display = 'block';"
                onmouseout="document.getElementById('junct').style.display='none';">
        
          <a href="http://frc.ri.cmu.edu/~kaess/pub/Wang15iros.pdf" id="SIRFS">
          <papertitle>Bridging text spotting and SLAM with junction features.</papertitle>
          </a>
          <br>
          <a href="http://people.csail.mit.edu/hchengwang/">Hsueh-Cheng Wang</a>,
          <i>Chelsea Finn</i>,
          <a href="http://people.csail.mit.edu/lpaull/">Liam Paull</a>,
          <a href="http://frc.ri.cmu.edu/~kaess/">Michael Kaess</a>,
          <a href="http://persci.mit.edu/people/rosenholtz">Ruth Rosenholtz</a>,
          <a href="http://people.csail.mit.edu/teller/">Seth Teller</a>,
          <a href="http://marinerobotics.mit.edu/john-j-leonard">John Leonard</a><br>
<em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2015:
        <br>
              <div id="junct" style="display:none">
          We develop a method that integrates text-spotting with simultaneous localization and mapping (SLAM), that determines loop closures using text in the environment.
              </div>
            </div><br>

        
            <div onmouseover="document.getElementById('mmqe').style.display = 'block';"
                onmouseout="document.getElementById('mmqe').style.display='none';">
          <a href="https://people.eecs.berkeley.edu/~dhm/papers/icra2015mmqe_final.pdf" id="3DSP">
          <papertitle>Beyond Lowest-Warping Cost Action Selection in Trajectory Transfer</papertitle>
          </a>
          <br>
          <a href="https://people.eecs.berkeley.edu/~dhm/">Dylan Hadfield-Menell</a>,
          <a href="https://people.eecs.berkeley.edu/~alexlee_gk/">Alex X. Lee</a>,
          <i>Chelsea Finn</i>,
          <a href="https://github.com/erictzeng">Eric Tzeng</a>,
          <a href="https://people.eecs.berkeley.edu/~shhuang/">Sandy Huang</a>,
          <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
          <br>
          <em>International Conference on Robotics and Automation (ICRA)</em>, 2015 
        <br>
              <div id="mmqe" style="display:none">
        We consider the problem of selecting which demonstration to transfer to the current test scenario.
        We frame the problem as an options Markov decision process (MDP) and develop an approach to learn a Q-function from expert demonstrations.
        Our results show significant improvement over nearest-neighbor selection.
              </div>
            </div><br>


        </td>

      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right"><font size="2">
          <a href="https://people.eecs.berkeley.edu/~barron/">This guy makes a nice webpage.</a>
          </font>
        </p>
        </td>
      </tr>
  </tbody></table>

  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-59618557-1', 'auto');
                ga('send', 'pageview');

              </script>

    </td>
    </tr>
  </tbody></table>


</body></html>
