<!DOCTYPE html>
<html>
  <head>
    <title>Zhiwu Huang, Computer Vision Lab, ETH Zurich </title>
    <link href='https://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109828585-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-109828585-1');
	</script>
	  
	<script type="text/javascript">
	   function visibility_on(id) {
		var e = document.getElementById(id+"_text");
		if(e.style.display == 'none')
		    e.style.display = 'block';
		var e = document.getElementById(id+"_img");
		if(e.style.display == 'none')
		    e.style.display = 'block';
	   }
	   function visibility_off(id) {
		var e = document.getElementById(id+"_text");
		if(e.style.display == 'block')
		    e.style.display = 'none';
		var e = document.getElementById(id+"_img");
		if(e.style.display == 'block')
		    e.style.display = 'none';
	   }
	   function toggle_visibility(id) {
	       var e = document.getElementById(id+"_text");
	       if(e.style.display == 'inline')
		  e.style.display = 'block';
	       else
		  e.style.display = 'inline';
	       var e = document.getElementById(id+"_img");
	       if(e.style.display == 'inline')
		  e.style.display = 'block';
	       else
		  e.style.display = 'inline';
	   }
	   function toggle_vis(id) {
	       var e = document.getElementById(id);
	       if (e.style.display == 'none')
		   e.style.display = 'inline';
	       else
		   e.style.display = 'none';
	   }
	</script>
	  
	  
	  
    <style>
      body {
      font-family: 'Vollkorn', sans-serif;
      }

      .subheading {
      font-size: 120%;
      }

      h2 {
      clear: left;
      margin-top: 1em;
      }
      
      h3 {
      clear: left;
      margin-top: 1em;
      }
      
      h4 {
      clear: left;
      margin-top: 1em;
      }
      
      img {
      float: left;
      height: 12em;
      margin-right: 1em;
      margin-bottom: 3em;
      }
     

      p {
      margin: 0.5em;
      }

      nav {
      margin-top: 3em;
      margin-bottom: 1em;
      }
        
        a:visited {color:#006633}
        a:link {color:#006633}
        a:hover {color: #FF0000}
        
        h1,h2,h3,h4{
            color:#006633
        }

    </style>
  </head>
  
  <body>
    <table cellspacing="0"><tr><td width=100%>
    <h1>Zhiwu Huang</h1>
    <div class="subheading">
      <img src="ZhiwuHuang-Hawaii.jpg"/>
      I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, ETH Zurich. My research interest lies in Computer Vision and Machine Learning for <b> Automated Video Artificial Intelligence, </b> capable of autmatically learning to understand the world through videos. I currently working on video generation, enhancement and manipulation as well as human-focussed video clustering, classification, prediction with deep manifold learning, generative distribution learning, and neural architecture learning.<br><br>	
      <b> [<a href="http://scholar.google.ch/citations?user=yh6t92AAAAAJ&hl=en">Google Scholar</a>] [<a href="https://github.com/zhiwu-huang"> Github </a>] </b>
      
       <!-- <b>Prospective students </b>, please <a href="javascript:toggle_vis('contact')">read this</a> before contacting me.
              <div id="contact" style="display:none"> 
                  Thank you for your interest in joining my research team! I am taking on new MS and PhD students each year. However, I ask that you do not contact me directly with regard to MS or PhD admissions until after you are admitted, as I will not be able to reply to individual emails. <br>
                  If you are interested in a <i>post-doc</i> position, please read <a href="https://goo.gl/forms/aKL2gnq8T80FNVMb2">this form</a>. <br>
                  If you are a current or admitted <i>Stanford undergraduate or MS student</i> interested in research positions, please read <a href="https://goo.gl/forms/uJuYOfIBQVPhEEDy1">this form</a>. <br>
                  If you are not a Stanford student and insteresed in research positions, please read <a href="https://goo.gl/forms/9scJRH3hw6z7GNbj2">this form</a>.
              </div>
        </p>-->
        
    </div>
    

    <nav>
      Jump to: 
      <a href="index.html#Projects">Research</a> |
      <a href="publication.html">Publications</a> |
      <a href="workshop.html">Workshops </a> |
      <a href="talk.html">Talks</a> |
      <a href="teaching.html">Teaching</a> |
      <a href="about.html">About me</a> | 
      <a href="contact.html">Contact</a>
    </nav>
	    
    <h2>News</h2>
                <li> 07/2020, One paper "Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search" is accepted by ECCV 2020. </li>
	 	<li> 05/2020, We are organizing <a href= "https://competitions.codalab.org/competitions/24685"> AIM Video Super-Resolution Challenge </a> @ECCV 2020. </li>
 		<li> 05/2020, One paper "NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results" will appear at the workshop NTIRE in conjunction with CVPR 2020. </li>
    		<li> 12/2019, We are organizing <a href= "https://competitions.codalab.org/competitions/20247"> NTIRE Video Quality Mapping Challenge </a> @CVPR 2020. </li>
    		<li> 10/2019, One dataset paper "The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping" will appear at the workshop AIM in ICCV 2019. </li>
		<li>10/2019, We are organizing a workshop <a href= "http://www.vision.ee.ethz.ch/aim19/"> "AIM: Advances in Image Manipulation Workshop and Challenges on Image and Video Manipulation" </a>
  and a tutorial <a href= "http://www.vision.ee.ethz.ch/fire19/index_fire19.html"> "FIRE: From Image Restoration to Enhancement and Beyond" </a> at ICCV, Oct. 27, 2019. </li>

		<li>02/2019, One paper "Sliced Wasserstein Generative Models" is accepted by CVPR 2019. This paper was selected as one of the <b> best publications </b> of the week (20.04.2019), by DeepAI. <a  href="https://deepai.org/publication/sliced-wasserstein-generative-models"> Link to DeepAI Website </a> </li>

		<li> <a href="javascript:toggle_vis('news')"> Older news...</a> </li>
		<div id="news" style="display:none"> 
			<li>11/2018, One paper "Manifold-valued Image Generation with Wasserstein Generative Adversarial Nets" is accepted by AAAI 2019. <em> This paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli (2018 Fields Medal winner) to generative modeling, which is able to generate photo-realistic biological samples. </em> </li>
	<!--<b> The paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli, who won 2018 Fields Medal. </b> -->
			<li>07/2018, One paper "Wasserstein Divergence for GANs" is accepted by ECCV 2018.</li>
			<li>04/2018, One paper "Covariance Pooling for Facial Expression Recognition" is accepted by the workshop DiffCVML in CVPR 2018.</li>
			<li>11/2017, One paper "Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video" is accepted as a Regular Paper in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</li>
			<li>11/2017, One paper "Building Deep Networks on Grassmann Manifolds" (GrNet) is accepted by AAAI 2018.</li>
		    <li>08/2017, One paper "Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets" is accepted by IEEE Transactions on Image Processing (TIP).</li>
			<li>07/2017, One paper "Geometry-aware Similarity Learning on SPD Manifolds for Visual Recognition" is accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
			<li>02/2017, One paper "Deep Learning on Lie Groups for Skeleton-based Action Recognition" (LieNet) is accepted as a spotlight by CVPR 2017. <em> This paper proposes deep networks of Lie Groups for skeleton-based action recognition. </em> </li>
		    <li>11/2016, One paper "A Riemannian Network for SPD Matrix Learning" (SPDNet) is accepted by AAAI 2017. <em>  This paper opens a new direction of deep manifold networks. </em> </li>
	     	</div>	

          </td>
    </tr></table>
    
    <a name="Projects"><h2>Research </h2></a>
	  
    <h3> Memembers </h3>
	   
	      <li> <a href="https://scholar.google.com/citations?user=wbk0QAcAAAAJ&hl=en"> Dr. Suryansh Kumar </a> (Postdoc researcher), Nov. 2019 - Present, with <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>

	      <li> <a href="https://scholar.google.ch/citations?user=EyE8nngAAAAJ&hl=de"> Anton Obukhov </a> (PhD candidate), Feb. 2020 - Present, with <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://scholar.google.com/citations?user=a5GBXkEAAAAJ&hl=en"> Stamatios Georgoulis </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>
	      <li> <a href="https://scholar.google.com/citations?user=84hs7J4AAAAJ&hl=en"> Yuan Tian </a>(PhD candidate), Feb. 2020 - Present, with <a href="https://scholar.google.com.sg/citations?user=yjG4Eg4AAAAJ&hl=en"> Wen Li </a>, <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink </a> </li>
	      <li> <a href="https://scholar.google.ch/citations?user=-8OUEKwAAAAJ&hl=en"> Qin Wang </a> (PhD candidate), Feb. 2020 - Present, with <a href="https://scholar.google.com.sg/citations?user=yjG4Eg4AAAAJ&hl=en"> Wen Li </a>, <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink </a> </li>

	      <li> <a href="https://scholar.google.com/citations?user=0nGkMM8AAAAJ&hl=en"> Mohamad Shahbazi </a> (PhD candidate), Nov. 2019 - Present, with <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=3BHMHU4AAAAJ&hl=en"> Ajad Chhatkuli </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>
	      <li> <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a> (PhD candidate), June 2019 - Present, with <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>  </li>
	      <li> <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a> (PhD, graduated), Nov. 2016 - March 2020, with <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>

    <h3>  Projects </h3>
          <h4>Video Generation</h4>
          <table><tr>
		  
		  <td width="40%">  <a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io"><video autoplay muted loop width="100%">
			  <!--<source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">-->
			  <source src="trailerfaces_generation_cropped.mp4" type="video/mp4">
			  <source src="trailerfaces_generation_cropped.mp4" type="video/mp4">
			  Sorry, we cannot display the generated video wall as
			  your browser doesn't support HTML5 video.
                  </video></a></td>
		  
		<td valign="top">
		<p><a href="https://github.com/musikisomorphie/swd"> Source Code </a> | <a href="https://data.vision.ee.ethz.ch/zzhiwu/trailerFaces-tfrecords.zip"> TrailerFace Dataset </a> 
	    	<p> Sliced Wasserstein Generative Models. <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu* </a>, <b>Zhiwu Huang*</b>,  <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <a href="https://scholar.google.com/citations?user=yjG4Eg4AAAAJ&hl=en"> Wen Li </a>, <a href="https://scholar.google.ch/citations?user=NpZj7TAAAAAJ&hl=en"> Janine Thoma </a>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em> (*indicates equal contributions) </em>                
			 <em> Computer Vision and Pattern Recognition (CVPR), 2019 </em>. <a href="https://arxiv.org/abs/1706.02631">Paper</a></p>
	        <p>Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs. <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.  
			 <em>  <a href="https://arxiv.org/abs/1810.02419"> arXiv:1810.02419 </a>, 2018. </em> </p>
		<p>Improving Video Generation for Multi-functional Applications. <a href="https://scholar.google.ch/citations?user=zYjOhRUAAAAJ&hl=en"> Bernhard Kratzwald </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>,  <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em>  <a href="https://arxiv.org/abs/1711.11453"> arXiv:1711.11453 </a>, 2017. <a href="https://bernhard2202.github.io/ivgan/index.html"> Project Page </a> </em> </p>
                <p>Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search. <a href="https://scholar.google.com/citations?user=84hs7J4AAAAJ&hl=en">Yuan Tian*</a>, <a href="https://scholar.google.ch/citations?user=-8OUEKwAAAAJ&hl=en">Qin Wang*</a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com.sg/citations?user=yjG4Eg4AAAAJ&hl=en">Wen Li</a>, <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://www.linkedin.com/in/minghaoyang/?originalSubdomain=nl"> Minghao Yang </a>, <a href="https://scholar.google.com/citations?user=wIE1tY4AAAAJ&hl=en"> Jun Wang </a>, <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink</a>. <em> (*indicates equal contributions) </em>. <em> European Conference on Computer Vision (ECCV), 2020 </em>. Accepted. <a href="">Paper</a> 

          </td></tr></table>
	  
     <!--<h3><a href="https://competitions.codalab.org/competitions/20247">Video Enhancement</a></h3>
	    <a href="ideo_enhancement_demo.jpg"><img src="video_enhancement_demo.jpg" width="480"></a>-->
          <h4>Video Enhancement</h4>
          <table><tr>
		  
	 <td valign="top" width="60%">
	  
	    <p><a href="https://competitions.codalab.org/competitions/20247"> Video Quality Mapping Challenge </a></p> 
	   <p> Divide-and-Conquer Adversarial Learning for High-Resolution Image and Video Enhancement.  <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, Guanju Li,  <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <a href="https://arxiv.org/abs/1910.10455"> arXiv:1910.10455 </a>, 2019.
	    </p>
	  <p> NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results. <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=sv"> Martin Danelljan, <a href="https://scholar.google.com/citations?user=-kSTt40AAAAJ&hl=en"> Shuhang Gu </a>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>. <em>  Computer Vision and Pattern Recognition (CVPR) workshop, 2020 </em>. <a href="https://arxiv.org/pdf/2005.02291.pdf">Paper</a>
	    </p>
	    <p> The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping. Sohyeong Kim, Guanju Li, <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=sv"> Martin Danelljan </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=-kSTt40AAAAJ&hl=en"> Shuhang Gu </a>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>. <em>  International Conference on Computer Vision (ICCV) workshop, 2019 </em>. <a href="http://www.vision.ee.ethz.ch/~timofter/publications/Kim-ICCVW-2019.pdf">Paper</a>
	    </p>
          </td>
		  
         <td width="40%">  <a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io"><video autoplay muted loop width="100%">
			  <!--<source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">-->
			  <source src="dacal_supp_vid3oc_converted.mp4" type="video/mp4">
			  <source src="dacal_supp_vid3oc_converted.mp4" type="video/mp4">
			  Sorry, we cannot display the generated video wall as
			  your browser doesn't support HTML5 video.
          </video></a></td>
		  
         
	  
	  </tr></table>
            
            
          <h4>Video Classification</h4>
          <table><tr>
		  
		  <td width="40%">  <a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io"><video autoplay muted loop width="100%">
			  <!--<source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">-->
			  <source src="video_classification_croped2.mp4" type="video/mp4">
			  <source src="video_classification_croped2.mp4" type="video/mp4">
			  Sorry, we cannot display the generated video wall as
			  your browser doesn't support HTML5 video.
                  </video></a></td>
		  
		<td valign="top">
		<p><a href="https://github.com/zhiwu-huang/LieNet"> Source Code </a> | <a href=" https://data.vision.ee.ethz.ch/zzhiwu/ManifoldNetData/LieData/G3D_Lie_data.zip"> G3D LieGroupData </a> 
	    	<p> A Riemannian Network for SPD Matrix Learning. <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.  
			 <em>  Association for the Advancement of Artificial Intelligence (AAAI), 2017. </em> <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14633/14371">Paper</a>|
			      <a href="https://github.com/zzhiwu/SPDNet-master"> Code </a> </p>
		<p> Deep Learning on Lie Groups for Skeleton-based Action Recognition. <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=bm5dZwIAAAAJ&hl=en"> Chengde Wan </a>, <a href="https://scholar.google.ch/citations?user=pfEoUpcAAAAJ&hl=de"> Thomas Probst </a>,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.                
			 <em> Computer Vision and Pattern Recognition (CVPR), 2017 </em>. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Deep_Learning_on_CVPR_2017_paper.pdf">Paper</a>|
			      <a href="https://github.com/zhiwu-huang/LieNet"> Code </a></p>
		<p>Building Deep Networks on Grassmann Manifolds. <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em> Association for the Advancement of Artificial Intelligence (AAAI),  2018. </em> <a href="https://arxiv.org/pdf/1611.05742.pdf">Paper</a>|
			      <a href="https://github.com/zzhiwu/GrNet"> Code </a> </p>
        </td></tr></table>
    
	    
   </body>
</html>
