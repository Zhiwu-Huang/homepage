<!DOCTYPE html>
<html>
  <head>
    <title>Zhiwu Huang, Computer Vision Lab, ETH Zurich </title>
    <link href='https://fonts.googleapis.com/css?family=Vollkorn' rel='stylesheet' type='text/css'>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-109828585-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-109828585-1');
	</script>
	  
	<script type="text/javascript">
	   function visibility_on(id) {
		var e = document.getElementById(id+"_text");
		if(e.style.display == 'none')
		    e.style.display = 'block';
		var e = document.getElementById(id+"_img");
		if(e.style.display == 'none')
		    e.style.display = 'block';
	   }
	   function visibility_off(id) {
		var e = document.getElementById(id+"_text");
		if(e.style.display == 'block')
		    e.style.display = 'none';
		var e = document.getElementById(id+"_img");
		if(e.style.display == 'block')
		    e.style.display = 'none';
	   }
	   function toggle_visibility(id) {
	       var e = document.getElementById(id+"_text");
	       if(e.style.display == 'inline')
		  e.style.display = 'block';
	       else
		  e.style.display = 'inline';
	       var e = document.getElementById(id+"_img");
	       if(e.style.display == 'inline')
		  e.style.display = 'block';
	       else
		  e.style.display = 'inline';
	   }
	   function toggle_vis(id) {
	       var e = document.getElementById(id);
	       if (e.style.display == 'none')
		   e.style.display = 'inline';
	       else
		   e.style.display = 'none';
	   }
	</script>
	  
	  
	  
    <style>
      body {
      font-family: 'Vollkorn', sans-serif;
      }

      .subheading {
      font-size: 120%;
      }

      h2 {
      clear: left;
      margin-top: 1em;
      }
      
      h3 {
      clear: left;
      margin-top: 1em;
      }
      
      h4 {
      clear: left;
      margin-top: 1em;
      }
      
      img {
      float: left;
      height: 12em;
      margin-right: 1em;
      margin-bottom: 3em;
      }

      p {
      margin: 0.5em;
      }

      nav {
      margin-top: 3em;
      margin-bottom: 1em;
      }
        
        a:visited {color:#006633}
        a:link {color:#006633}
        a:hover {color: #FF0000}
        
        h1,h2,h3,h4{
            color:#006633
        }

    </style>
  </head>
  
  <body>
    <table cellspacing="0"><tr><td width=100%>
    <h1>Zhiwu Huang</h1>
    <div class="subheading">
      <!--<img src="./Photos/ZhiwuHuang-Hawaii.jpeg"/>-->
      <img src="./Photos/ZhiwuHuang-Sydney_2x-9.jpg"/>
      <!--I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html"> ETH Zurich </a>. My research interest lies in Computer Vision and Machine Learning for Automated Video Artificial Intelligence, capable of automatically learning to understand the world through videos. My research includes applications to deepfakes tracking, affective computing, and autonomous driving. <br><br>-->	
      <!--I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html"> ETH Zurich </a>. My research studies human-inspired visual intelligence to understand the world through videos, with applications to deepfakes generation & detection, perceptual quality enhancing, affective & behavioural computing, as well as scalable autonomous driving. <br><br>-->
      <!--I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html"> ETH Zurich </a>. My research studies human-inspired visual intelligence through automated machine learning, with applications to deepfakes generation & detection, perceptual quality enhancing, affective & behavioural computing, as well as scalable autonomous driving. <br><br>-->
      <!--I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html"> ETH Zurich </a>. My research team (AutoLV, Automated Learning in Vision) studies human-inspired visual intelligence through automated machine learning on data, label, feature, neuron and task. The applications include visual deepfake, affect and behavior computing. <br><br>-->
      <!--I am a postdoctoral researcher with Prof. <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a> at <a href="http://www.vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html"> ETH Zurich </a>. My research team (AutoLV) studies automated learning in vision that aims for making machines to learn the visual world, all by themselves. The current research focus is on visual fake, affect and behavior computing through automated machine learning on data, label, feature, neuron and task. <br><br> -->
    	    
      <!--I currently working on video generation, enhancement and manipulation as well as human-focussed video clustering, classification, prediction with deep manifold learning, generative distribution learning, and neural architecture learning.-->
      I will be starting as an Assistant Professor in the <a href="https://sis.smu.edu.sg"> School of Computing and Information Systems <a> at <a href="https://www.smu.edu.sg"> Singapore Management University (SMU) </a> in Summer of 2021. My research team (<a href="index.html#Projects"> AutoLV </a>) studies automated learning in vision that aims for making machines to learn the visual world, all by themselves. The current research focus is on visual fake, affect and behavior computing through automated machine learning on data, label, feature, neuron and task. Please follow the <a href="position.html"> instructions </a> to reach me if you are interested in joining my research group as a PhD student. <br>
	     
      <!--<b> [<a href="http://scholar.google.ch/citations?user=yh6t92AAAAAJ&hl=en">Google Scholar</a>] [<a href="https://github.com/zhiwu-huang"> Github </a>] </b> -->
      
       <!-- <b>Prospective students </b>, please <a href="javascript:toggle_vis('contact')">read this</a> before contacting me.
              <div id="contact" style="display:none"> 
                  Thank you for your interest in joining my research team! I am taking on new MS and PhD students each year. However, I ask that you do not contact me directly with regard to MS or PhD admissions until after you are admitted, as I will not be able to reply to individual emails. <br>
                  If you are interested in a <i>post-doc</i> position, please read <a href="https://goo.gl/forms/aKL2gnq8T80FNVMb2">this form</a>. <br>
                  If you are a current or admitted <i>ETH MS student</i> interested in research positions, please read <a href="https://goo.gl/forms/uJuYOfIBQVPhEEDy1">this form</a>. <br>
                  If you are not an ETH student and insteresed in research positions, please read <a href="https://goo.gl/forms/9scJRH3hw6z7GNbj2">this form</a>.
              </div>
        </p>-->
        
    </div>
    

    <nav>
      Jump to: 
      <a href="index.html#Projects">Research</a> |
      <a href="publication.html">Publications</a> |
      <a href="workshop.html">Workshops </a> |
      <a href="talk.html">Talks</a> |
      <a href="teaching.html">Teaching</a> |
      <a href="position.html">Opening positions</a> | 
      <a href="about.html">About me</a> | 
      <a href="contact.html">Contact</a>
    </nav>
	    
    <h2>News</h2>
	        <li> 04/2021, One paper <a href="https://arxiv.org/pdf/2010.14535.pdf"> "Neural Architecture Search of SPD Manifold Networks" </a> is accepted by IJCAI 2021. </li>	    	    
	        <li> 03/2021, Two papers <a href="https://arxiv.org/pdf/2102.06696.pdf"> "Efficient Conditional GAN Transfer with Knowledge Propagation across Classes" </a> and <a href="./Papers/GANmut_CVPR2021.pdf">"GANmut: Learning Interpretable Conditional Space for Gamut of Emotions" </a> are accepted by CVPR 2021. </li>	    	    
	        <li> 01/2021, One paper <a href="https://arxiv.org/pdf/2103.04217.pdf"> "Spectral Tensor Train Parameterization of Deep Learning Layers" </a> is accepted by AISTATS 2021. </li>	    	    
	        <li> 12/2020, One paper <a href="https://arxiv.org/pdf/2007.16112.pdf"> "Neural Architecture Search as Sparse Supernet" </a> is accepted by AAAI 2021. </li>	    	    
	        <li> 11/2020, One paper <a href="https://arxiv.org/pdf/2010.09849.pdf"> "Facial Emotion Recognition with Noisy Multi-task Annotations" </a> is accepted by WACV 2021. </li>
	        <li> 08/2020, One paper <a href="./Papers/ECCV_2020__AIM2020_VXSR.pdf"> "AIM 2020 Challenge on Video Extreme Super-Resolution: Methods and Results" </a> will appear at the workshop AIM in conjunction with ECCV 2020. </li>
	        <li> 07/2020, One paper <a href="./Papers/BMVC2020_WeaklyPaired.pdf"> "Weakly Paired Multi-Domain Image Translation" </a> is accepted by BMVC 2020 as an oral. </li>
	        <li> 07/2020, One paper <a href= "https://arxiv.org/pdf/2007.09180.pdf"> "Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search" </a> is accepted by ECCV 2020. </li>
	 	<li> 05/2020, We are organizing <a href= "https://competitions.codalab.org/competitions/24685"> AIM Video Super-Resolution Challenge </a> @ECCV 2020. </li>
 		<li> 05/2020, One paper <a href="https://arxiv.org/pdf/2005.02291.pdf">"NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results" </a> will appear at the workshop NTIRE in conjunction with CVPR 2020. </li>
    		<li> 12/2019, We are organizing <a href= "https://competitions.codalab.org/competitions/20247"> NTIRE Video Quality Mapping Challenge </a> @CVPR 2020. </li>
    		<li> 10/2019, One dataset paper <a href="http://www.vision.ee.ethz.ch/~timofter/publications/Kim-ICCVW-2019.pdf"> "The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping" </a> will appear at the workshop AIM in ICCV 2019. </li>
		<li>10/2019, We are organizing a workshop <a href= "http://www.vision.ee.ethz.ch/aim19/"> "AIM: Advances in Image Manipulation Workshop and Challenges on Image and Video Manipulation" </a>
  and a tutorial <a href= "http://www.vision.ee.ethz.ch/fire19/index_fire19.html"> "FIRE: From Image Restoration to Enhancement and Beyond" </a> at ICCV, Oct. 27, 2019. </li>

		<li>02/2019, One paper <a href="https://arxiv.org/pdf/1706.02631.pdf">"Sliced Wasserstein Generative Models" </a> is accepted by CVPR 2019. This paper was selected as one of the <b> best publications </b> of the week (20.04.2019), by DeepAI. <a  href="https://deepai.org/publication/sliced-wasserstein-generative-models"> Link to DeepAI Website </a> </li>

		<li> <a href="javascript:toggle_vis('news')"> Older news...</a> </li>
		<div id="news" style="display:none"> 
			<li>11/2018, One paper "Manifold-valued Image Generation with Wasserstein Generative Adversarial Nets" is accepted by AAAI 2019. <em> This paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli (2018 Fields Medal winner) to generative modeling, which is able to generate photo-realistic biological samples. </em> </li>
	<!--<b> The paper applies the theory of optimal transport on non-compact manifolds from Alessio Figalli, who won 2018 Fields Medal. </b> -->
			<li>07/2018, One paper "Wasserstein Divergence for GANs" is accepted by ECCV 2018.</li>
			<li>04/2018, One paper "Covariance Pooling for Facial Expression Recognition" is accepted by the workshop DiffCVML in CVPR 2018.</li>
			<li>11/2017, One paper "Cross Euclidean-to-Riemannian Metric Learning with Application to Face Recognition from Video" is accepted as a Regular Paper in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</li>
			<li>11/2017, One paper "Building Deep Networks on Grassmann Manifolds" (GrNet) is accepted by AAAI 2018.</li>
		    <li>08/2017, One paper "Discriminant Analysis on Riemannian Manifold of Gaussian Distributions for Face Recognition with Image Sets" is accepted by IEEE Transactions on Image Processing (TIP).</li>
			<li>07/2017, One paper "Geometry-aware Similarity Learning on SPD Manifolds for Visual Recognition" is accepted by IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
			<li>02/2017, One paper "Deep Learning on Lie Groups for Skeleton-based Action Recognition" (LieNet) is accepted as a spotlight by CVPR 2017. <em> This paper proposes deep networks of Lie Groups for skeleton-based action recognition. </em> </li>
		    <li>11/2016, One paper "A Riemannian Network for SPD Matrix Learning" (SPDNet) is accepted by AAAI 2017. <em>  This paper opens a new direction of deep manifold networks. </em> </li>
	     	</div>	

          </td>
    </tr></table>
    
    <a name="Projects"><h2>Research </h2></a>
	  
    <h3> Memembers (/Collaborators) </h3>
	   
	      <li> <a href="https://scholar.google.com/citations?user=wbk0QAcAAAAJ&hl=en"> Suryansh Kumar </a> (Postdoc researcher), Nov. 2019 - Present, with <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>

	      <li> <a href="https://scholar.google.ch/citations?user=EyE8nngAAAAJ&hl=de"> Anton Obukhov </a> (PhD candidate), Feb. 2020 - Present, with <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://scholar.google.com/citations?user=a5GBXkEAAAAJ&hl=en"> Stamatios Georgoulis </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>
	      <li> <a href="https://scholar.google.com/citations?user=84hs7J4AAAAJ&hl=en"> Yuan Tian </a>(PhD candidate), Feb. 2020 - Present, with <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink </a> </li>
	      <li> <a href="https://scholar.google.ch/citations?user=-8OUEKwAAAAJ&hl=en"> Qin Wang </a> (PhD candidate), Feb. 2020 - Present, with <a href="https://scholar.google.com.sg/citations?user=yjG4Eg4AAAAJ&hl=en"> Wen Li </a>, <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink </a> </li>

	      <li> <a href="https://scholar.google.com/citations?user=0nGkMM8AAAAJ&hl=en"> Mohamad Shahbazi </a> (PhD candidate), Nov. 2019 - Present, with <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=3BHMHU4AAAAJ&hl=en"> Ajad Chhatkuli </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>
	      <li> <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a> (PhD candidate), June 2019 - Present, with <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=sv"> Martin Danelljan </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>  </li>
	      <li> <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a> (PhD, graduated, <a href="https://www.research-collection.ethz.ch/handle/20.500.11850/414485"> Thesis </a>), Nov. 2016 - March 2020, with <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a> </li>

    <!--<h3>  Major Projects </h3>-->
    <h3> Projects </h3>

          <!--<h4>Video Generation</h4>-->
	  <h4> Adversarial Fake Computing </h4>
          <table><tr>
		  
		  <td width="40%">  <a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io"><video autoplay muted loop width="100%">
			  <!--<source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">-->
			  <source src="./Videos/trailerfaces_generation_cropped.mp4" type="video/mp4">
			  <source src="./Videos/trailerfaces_generation_cropped.mp4" type="video/mp4">
			  Sorry, we cannot display the generated video wall as
			  your browser doesn't support HTML5 video.
                  </video></a></td>
		  
		<td valign="top">
		<p><a href="https://github.com/musikisomorphie/swd"> Source Code </a> | <a href="https://data.vision.ee.ethz.ch/zzhiwu/trailerFaces-tfrecords.zip"> TrailerFace Dataset </a> 
	    	<p> Sliced Wasserstein Generative Models. <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu* </a>, <b>Zhiwu Huang*</b>, Dinesh Acharya, <a href="https://scholar.google.com/citations?user=yjG4Eg4AAAAJ&hl=en"> Wen Li </a>, <a href="https://scholar.google.ch/citations?user=NpZj7TAAAAAJ&hl=en"> Janine Thoma </a>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em> (*indicates equal contributions). </em>                
			 In <em> Computer Vision and Pattern Recognition (CVPR), 2019 </em>. <a href="https://arxiv.org/pdf/1706.02631.pdf">Paper</a> <br>
			<b>Extension:</b> Towards High Resolution Video Generation with Progressive Growing of Sliced Wasserstein GANs. Dinesh Acharya, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.  
			 <em> arXiv preprint arXiv:1810.02419, 2018. </em> <a href="https://arxiv.org/pdf/1810.02419.pdf"> Paper </a> | <a href="https://github.com/musikisomorphie/swd"> Code </a> <b> (Automated Data Learning) </b></p>
		<!--<p>Improving Video Generation for Multi-functional Applications. <a href="https://scholar.google.ch/citations?user=zYjOhRUAAAAJ&hl=en"> Bernhard Kratzwald </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>,  <a href="https://www.researchgate.net/scientific-contributions/2142556529_Dinesh_Acharya"> Dinesh Acharya </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em>  arXiv:1711.11453, 2017</em>. <a href="https://arxiv.org/pdf/1711.11453.pdf"> Paper </a> | <a href="https://bernhard2202.github.io/ivgan/index.html"> Project Page </a> </p> -->
                <!--<p> Wasserstein Divergence for GANs. <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=NpZj7TAAAAAJ&hl=en"> Janine Thoma </a>, Dinesh Acharya, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. In <em> European Conference on Computer Vision (ECCV), 2018  </em>.  <a href="https://arxiv.org/pdf/1712.01026.pdf">Paper</a> | <a href="https://github.com/musikisomorphie/wgan-div">Code</a> <b> (Automated Label Learning) </b> -->
		<p>Off-Policy Reinforcement Learning for Efficient and Effective GAN Architecture Search. <a href="https://scholar.google.com/citations?user=84hs7J4AAAAJ&hl=en">Yuan Tian*</a>, <a href="https://scholar.google.ch/citations?user=-8OUEKwAAAAJ&hl=en">Qin Wang*</a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com.sg/citations?user=yjG4Eg4AAAAJ&hl=en">Wen Li</a>, <a href="https://scholar.google.ch/citations?user=T51W57YAAAAJ&hl=en"> Dengxin Dai </a>, <a href="https://www.linkedin.com/in/minghaoyang/?originalSubdomain=nl"> Minghao Yang </a>, <a href="https://scholar.google.com/citations?user=wIE1tY4AAAAJ&hl=en"> Jun Wang </a>, <a href="https://scholar.google.com/citations?user=eAcIoUgAAAAJ&hl=en"> Olga Fink</a>. <em> (*indicates equal contributions) </em>. In <em> European Conference on Computer Vision (ECCV), 2020 </em>. <a href="https://arxiv.org/pdf/2007.09180.pdf">Paper </a> | <a href="https://github.com/Yuantian013/E2GAN"> Code </a> <b> (Automated Neuron Learning) </b>  
		<p>Efficient Conditional GAN Transfer with Knowledge Propagation across Classes. Mohamad Shahbazi, <b>Zhiwu Huang</b>, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool. In <em> Computer Vision and Pattern Recognition (CVPR), 2021. </em> <a href="https://arxiv.org/pdf/2102.06696.pdf"> Preprint </a> | <a href="https://github.com/mshahbazi72/cGANTransfer"> Code </a> <b> (Automated Task Learning) </b>   </p>

          </td></tr></table>
	    
	   <!--<h3>  Other Projects </h3>-->

	  <!--<h3><a href="https://competitions.codalab.org/competitions/20247">Video Enhancement</a></h3>
	    <a href="ideo_enhancement_demo.jpg"><img src="video_enhancement_demo.jpg" width="480"></a>-->
          <!--<h4>Video Enhancement</h4>-->
	    
	 <!--<h4> Perceptual Quality Enhancing </h4>-->
          <table><tr>
		  
         <td width="40%">  <a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io"><video autoplay muted loop width="100%">
			  <!--<source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">-->
			  <source src="./Videos/dacal_supp_vid3oc_converted.mp4" type="video/mp4">
			  <source src="./Videos/dacal_supp_vid3oc_converted.mp4" type="video/mp4">
			  Sorry, we cannot display the generated video wall as
			  your browser doesn't support HTML5 video.
          </video></a></td>
		  
	 <td valign="top" width="60%">
	  
	    <p><a href="https://competitions.codalab.org/competitions/20247"> Video Quality Mapping Challenge </a> | <a href= "https://competitions.codalab.org/competitions/24685"> Video Super-Resolution Challenge </a> </p> 
            <p> The Vid3oC and IntVID Datasets for Video Super Resolution and Quality Mapping. Sohyeong Kim, Guanju Li, <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=sv"> Martin Danelljan </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=-kSTt40AAAAJ&hl=en"> Shuhang Gu </a>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>. In <em>  International Conference on Computer Vision (ICCV) workshop, 2019 </em>. <a href="http://www.vision.ee.ethz.ch/~timofter/publications/Kim-ICCVW-2019.pdf">Paper</a>
	    </p>
	    <p> Divide-and-Conquer Adversarial Learning for High-Resolution Image and Video Enhancement. <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <a href="https://github.com/ligua"> Guanju Li </a>,  <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em> arXiv preprint arXiv:1910.10455, 2019 </em>. <a href="https://arxiv.org/pdf/1910.10455.pdf"> Paper </a>
	    </p>
	    <p> NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results. <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=NCSSpMkAAAAJ&hl=sv"> Martin Danelljan, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a> and et al. In <em>  Computer Vision and Pattern Recognition (CVPR) workshop, 2020 </em>. <a href="https://arxiv.org/pdf/2005.02291.pdf">Paper</a>
	    </p>
            <p> AIM 2020 Challenge on Video Extreme Super-Resolution: Methods and Results. <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a> and et al. In <em> European Conference on Computer Vision (ECCV) workshop, 2020 </em>. <a href="./Papers/ECCV_2020__AIM2020_VXSR.pdf"> Paper </a>
	    </p>
	    <p> <a href="https://scholar.google.com/citations?user=PaIVtpMAAAAJ&hl=en"> Dario Fuoli </a>, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>, <a href="https://scholar.google.com/citations?user=u3MwH5kAAAAJ&hl=en"> Radu Timofte </a>. An Efficient Recurrent Adversarial Framework for Unsupervised Real-Time Video Enhancement. <em> arXiv preprint arXiv:2012.13033, 2020 </em>. <a href="https://arxiv.org/pdf/2012.13033.pdf"> Paper </a> </p>


          </td>
	  
	  </tr></table> 
	    
	    
	  <h4> Facial Affective Computing </h4>
          <table><tr>
		  
		  <td width="40%"> <img width="98%" src="./Images/FacialAffectComputing.png"/> </td>

		  
		  <td valign="top">
		<p><a href="https://github.com/zhiwu-huang/SPDNet"> Source Code </a> | <a href="https://data.vision.ee.ethz.ch/zzhiwu/ManifoldNetData/SPDData/AFEW_SPD_data.zip"> AFEW SPDData </a> 
			
		<p> A Riemannian Network for SPD Matrix Learning. <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.  
			 In <em>  Association for the Advancement of Artificial Intelligence (AAAI), 2017. </em> <a href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/14633/14371">Paper</a> |
			      <a href="https://github.com/zhiwu-huang/SPDNet"> Code </a> <b> (Automated Feature Learning) </b> <br>
	        <b> Extension:</b> Covariance Pooling for Facial Expression Recognition. Dinesh Acharya, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. In <em> Computer Vision and Pattern Recognition (CVPR) workshop, 2018 </em>. <a href="https://arxiv.org/pdf/1805.04855.pdf">Paper</a> | <a href="https://github.com/d-acharya/CovPoolFER">Code</a> 
		</p>
		  
	        <p>  Neural Architecture Search of SPD Manifold Networks. Rhea Sanjay Sukthanker, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=wbk0QAcAAAAJ&hl=en"> Suryansh Kumar </a>, Erik Goron Endsjo, Yan Wu,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. In <em> International Joint Conference on Artificial Intelligence (IJCAI), 2021. </em> <a href="https://arxiv.org/pdf/2010.14535.pdf"> Paper </a>  <b> (Automated Neuron Learning) </b> </p>  


		  
		
		
           </td></tr></table>
	    
	   <table><tr>
		  
		  <td width="40%"> <img width="88%" src="./Images/FacialAffectGamut.png"/> </td>

		  
		  <td valign="top">
			<p> Facial Emotion Recognition with Noisy Multi-task Annotations. Siwei Zhang, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. In <em> Winter Conference on Applications of Computer Vision (WACV), 2021 </em>.  <a href="https://arxiv.org/pdf/2010.09849.pdf"> Paper </a> | <a href="https://github.com/sanweiliti/noisyFER"> Code </a> <b> (Automated Label Learning) </b> </p>  


			<p> GANmut: Learning Interpretable Conditional Space for Gamut of Emotions. Stefano D'Apolito, Danda Pani Paudel, <b>Zhiwu Huang</b>, Andres Romero Vergara, Luc Van Gool. In <em> Computer Vision and Pattern Recognition (CVPR), 2021</em>. <a href="./Papers/GANmut_CVPR2021.pdf"> Preprint </a> | Project Page (coming soon) | Code (coming soon) <b> (Automated Label Learning) </b> </p>
  
		
		
           </td></tr></table>
	    
	    
	   <!--<h4>Video Classification</h4>-->
	  <h4> Skeletal Behavior Computing </h4>
          <table><tr>
		  
		<td width="40%">  <a href="https://github.com/zhiwu-huang/zhiwu-huang.github.io"><video autoplay muted loop width="100%">
			  <!--<source src="https://epic-kitchens.github.io/static/videos/04x04_vp9.webm" type="video/webm">-->
			  <source src="./Videos/video_classification_croped2.mp4" type="video/mp4">
			  <source src="./Videos/video_classification_croped2.mp4" type="video/mp4">
			  Sorry, we cannot display the generated video wall as
			  your browser doesn't support HTML5 video.
                </video></a></td>
		  
		<td valign="top" width="60%">
		<p><a href="https://github.com/zhiwu-huang/LieNet"> Source Code </a> | <a href=" https://data.vision.ee.ethz.ch/zzhiwu/ManifoldNetData/LieData/G3D_Lie_data.zip"> G3D LieGroupData </a> 			
		
		<p> Deep Learning on Lie Groups for Skeleton-based Action Recognition. <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=bm5dZwIAAAAJ&hl=en"> Chengde Wan </a>, <a href="https://scholar.google.ch/citations?user=pfEoUpcAAAAJ&hl=de"> Thomas Probst </a>,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>.                
			In <em> Computer Vision and Pattern Recognition (CVPR), 2017 </em>. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Deep_Learning_on_CVPR_2017_paper.pdf">Paper</a> |
			      <a href="https://github.com/zhiwu-huang/LieNet"> Code </a> | <a href="./Talks/lienet_spotlight_v1.0.pdf"> Spotlight </a> <b> (Automated Feature Learning) </b>  <br>
		<b> Extension:</b> Studying Multiple Cues for Human Action Recognition on RGB+D Data. Jonathan Gan, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a> <em> Tech. Report (Semester Thesis), 2017.	</em>  <a href="./Papers/MultiCuesActivity_JonGAN_thesis.pdf">Paper</a>
	        </p>
		
	        <p>Building Deep Networks on Grassmann Manifolds. <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=BCKKfUEAAAAJ&hl=en"> Jiqing Wu </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. In <em> Association for the Advancement of Artificial Intelligence (AAAI),  2018. </em> <a href="https://arxiv.org/pdf/1611.05742.pdf">Paper</a> |
			      <a href="https://github.com/zzhiwu/GrNet"> Code </a> <b> (Automated Feature Learning) </b>  </p>
	       

                </td>
		  
		
		  
	 </tr></table>
	    
	 <table><tr>
		  
		  <td width="40%"> <img width="100%" src="./Images/SkeletonEstimate.png"/> </td>

		  
		  <td valign="top">  
			  
			   
	                <p>  Neural Architecture Search of SPD Manifold Networks. Rhea Sanjay Sukthanker, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=wbk0QAcAAAAJ&hl=en"> Suryansh Kumar </a>, Erik Goron Endsjo, Yan Wu,  <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. In <em> International Joint Conference on Artificial Intelligence (IJCAI), 2021. </em> <a href="https://arxiv.org/pdf/2010.14535.pdf"> Paper </a>  <b> (Automated Neuron Learning) </b>  
			<b> Extension:</b> Neural Architecture Search on Lie Groups for Skeleton-based Action Recognition. Samuele Serafino, <b>Zhiwu Huang</b>, <a href="https://scholar.google.com/citations?user=wbk0QAcAAAAJ&hl=en"> Suryansh Kumar </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. <em> Tech. Report (Semester Thesis), 2020.	</em>  <a href="./Papers/LieNetNAS_thesis_Samuele.pdf">Paper</a>
			</p>  
		
		
           </td></tr></table>
	    
	    
	 
	    
	   
	  <!--<h4> Scalable Autonomous Driving </h4>
          <table><tr>
		  
		  <td width="40%"> <img height="120%" width="100%" src="./Images/DrivingSceneTranslation.png"/> </td>

		  
		  <td valign="top">
		<p><a href="https://github.com/zhangma123/weaklypaired"> Source Code (coming soon) </a> | <a href="https://github.com/zhangma123/weaklypaired"> Weakly-Paired Oxford Robotcar Dataset (coming soon) </a> 
	    
		  
		<p> Marc Yanlong Zhang, <b>Zhiwu Huang</b>, <a href="https://scholar.google.ch/citations?user=W43pvPkAAAAJ&hl=en"> Danda Pani Paudel </a>, <a href="https://scholar.google.ch/citations?user=NpZj7TAAAAAJ&hl=en"> Janine Thoma </a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en"> Luc Van Gool </a>. Weakly Paired Multi-Domain Image Translation. In <em> British Machine Vision Conference (BMVC), 2020. (Oral). </em> <a href="./Papers//BMVC2020_WeaklyPaired.pdf"> Paper </a> | <a href="./Papers//BMVC2020_WeaklyPaired_suppl.pdf"> Supp </a> | <a href="https://www.bmvc2020-conference.com/conference/papers/paper_0841.html"> Oral Presentation </a> <b> (Automated Data Learning) </b>   </p>

		
           </td></tr></table>  -->
	    
	   
	    
	    

    
	    
   </body>
</html>
